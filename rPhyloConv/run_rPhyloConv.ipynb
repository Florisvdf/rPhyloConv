{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import struct\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from numpy import unique\n",
    "import pandas as pd\n",
    "from array import array as pyarray\n",
    "from scipy import sparse, interp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, mean_absolute_error,accuracy_score, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, StratifiedShuffleSplit, ShuffleSplit, GridSearchCV, ParameterGrid, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, matthews_corrcoef, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin\n",
    "\n",
    "from Bio import Phylo\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from copy import deepcopy\n",
    "\n",
    "from models import build_model\n",
    "from utils.io import group_by_params\n",
    "from utils.treebuilding import TreeBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(256)\n",
    "random_seed = 42\n",
    "n_shuffles = 3\n",
    "test_data_ratio=0.2\n",
    "n_cv_folds = 5\n",
    "train_ratio = 1\n",
    "\n",
    "tree_path = \"tree.tree\" \n",
    "\n",
    "path = str(np.loadtxt(\"path.txt\", dtype = str))\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"output_data\")\n",
    "except OSError:\n",
    "    print(\"Directory already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"{}/X.csv\".format(path), index_col = 0)\n",
    "y = np.load(\"{}/y.npy\".format(path)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np = np.load(\"{}/X.npy\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = {0:'aTPO Negative', 1:'aTPO Positive'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_dict = {}\n",
    "auc_dict[\"TPR\"] = []\n",
    "auc_dict[\"FPR\"] = []\n",
    "auc_dict[\"Thresholds\"] = []\n",
    "auc_dict[\"AUC\"] = []\n",
    "\n",
    "list_auc=[]\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "shuffle_counter = 0\n",
    "\n",
    "StratShufSpl = StratifiedShuffleSplit(n_shuffles,\n",
    "                                    test_size = test_data_ratio, \n",
    "                                    random_state = random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(286, 449)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "----------------------------------------------------------------\n",
      "Beginning stability selection iteration 0\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Performing GridSearchCV for 5 candidates\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Beginning cross validation for candidate number 0\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Total size of the training set: 182\n",
      "Total size of the validation set: 46\n",
      "Class frequencies in the training set: [0.5 0.5]\n",
      "Class frequencies in the validation set: [0.5 0.5]\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Beginning the tree building procedure\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Fitting candidate number 1 in shuffle 1 with parameters\n",
      "\n",
      "{'activation': 'elu', 'batch_size': 64, 'callbacks': [<tensorflow.python.keras.callbacks.EarlyStopping object at 0x000002312EDC0508>], 'dropout': 0.3, 'epochs': 2000, 'filters': 100, 'input_shape': None, 'kernel_size': (3, 5), 'learning_rate': 0.001, 'loss': 'mse', 'n_classes': 2, 'n_layers': 2, 'pool_size': (2, 2), 'verbose': 2}\n",
      "Train on 182 samples, validate on 46 samples\n",
      "Epoch 1/2000\n",
      "182/182 - 2s - loss: 0.2504 - val_loss: 0.2488\n",
      "Epoch 2/2000\n",
      "182/182 - 1s - loss: 0.2483 - val_loss: 0.2471\n",
      "Epoch 3/2000\n",
      "182/182 - 1s - loss: 0.2461 - val_loss: 0.2453\n",
      "Epoch 4/2000\n",
      "182/182 - 1s - loss: 0.2435 - val_loss: 0.2429\n",
      "Epoch 5/2000\n",
      "182/182 - 1s - loss: 0.2411 - val_loss: 0.2394\n",
      "Epoch 6/2000\n",
      "182/182 - 1s - loss: 0.2398 - val_loss: 0.2364\n",
      "Epoch 7/2000\n",
      "182/182 - 1s - loss: 0.2357 - val_loss: 0.2334\n",
      "Epoch 8/2000\n",
      "182/182 - 1s - loss: 0.2329 - val_loss: 0.2322\n",
      "Epoch 9/2000\n",
      "182/182 - 1s - loss: 0.2233 - val_loss: 0.2287\n",
      "Epoch 10/2000\n",
      "182/182 - 1s - loss: 0.2268 - val_loss: 0.2259\n",
      "Epoch 11/2000\n",
      "182/182 - 1s - loss: 0.2165 - val_loss: 0.2244\n",
      "Epoch 12/2000\n",
      "182/182 - 1s - loss: 0.2176 - val_loss: 0.2197\n",
      "Epoch 13/2000\n",
      "182/182 - 1s - loss: 0.2090 - val_loss: 0.2179\n",
      "Epoch 14/2000\n",
      "182/182 - 1s - loss: 0.2020 - val_loss: 0.2147\n",
      "Epoch 15/2000\n",
      "182/182 - 2s - loss: 0.2017 - val_loss: 0.2135\n",
      "Epoch 16/2000\n",
      "182/182 - 1s - loss: 0.1973 - val_loss: 0.2107\n",
      "Epoch 17/2000\n",
      "182/182 - 1s - loss: 0.1878 - val_loss: 0.2104\n",
      "Epoch 18/2000\n",
      "182/182 - 1s - loss: 0.1946 - val_loss: 0.2097\n",
      "Epoch 19/2000\n",
      "182/182 - 1s - loss: 0.1857 - val_loss: 0.2101\n",
      "Epoch 20/2000\n",
      "182/182 - 1s - loss: 0.1846 - val_loss: 0.2074\n",
      "Epoch 21/2000\n",
      "182/182 - 1s - loss: 0.1655 - val_loss: 0.2058\n",
      "Epoch 22/2000\n",
      "182/182 - 1s - loss: 0.1706 - val_loss: 0.2038\n",
      "Epoch 23/2000\n",
      "182/182 - 1s - loss: 0.1542 - val_loss: 0.2054\n",
      "Epoch 24/2000\n",
      "182/182 - 1s - loss: 0.1564 - val_loss: 0.2055\n",
      "Epoch 25/2000\n",
      "182/182 - 1s - loss: 0.1544 - val_loss: 0.2002\n",
      "Epoch 26/2000\n",
      "182/182 - 1s - loss: 0.1501 - val_loss: 0.1999\n",
      "Epoch 27/2000\n",
      "182/182 - 1s - loss: 0.1386 - val_loss: 0.2009\n",
      "Epoch 28/2000\n",
      "182/182 - 1s - loss: 0.1359 - val_loss: 0.2016\n",
      "Epoch 29/2000\n",
      "182/182 - 1s - loss: 0.1325 - val_loss: 0.2039\n",
      "Epoch 30/2000\n",
      "182/182 - 1s - loss: 0.1399 - val_loss: 0.2063\n",
      "Epoch 31/2000\n",
      "182/182 - 1s - loss: 0.1247 - val_loss: 0.2063\n",
      "Epoch 32/2000\n",
      "182/182 - 1s - loss: 0.1212 - val_loss: 0.2061\n",
      "Epoch 33/2000\n",
      "182/182 - 1s - loss: 0.1219 - val_loss: 0.2065\n",
      "Epoch 34/2000\n",
      "182/182 - 1s - loss: 0.1165 - val_loss: 0.2059\n",
      "Epoch 35/2000\n",
      "182/182 - 1s - loss: 0.1049 - val_loss: 0.2060\n",
      "Epoch 36/2000\n",
      "182/182 - 1s - loss: 0.1129 - val_loss: 0.2073\n",
      "Epoch 37/2000\n",
      "182/182 - 1s - loss: 0.1071 - val_loss: 0.2097\n",
      "Epoch 38/2000\n",
      "182/182 - 1s - loss: 0.0925 - val_loss: 0.2080\n",
      "Epoch 39/2000\n",
      "182/182 - 1s - loss: 0.0971 - val_loss: 0.2088\n",
      "Epoch 40/2000\n",
      "182/182 - 1s - loss: 0.0896 - val_loss: 0.2109\n",
      "Epoch 41/2000\n",
      "182/182 - 1s - loss: 0.0962 - val_loss: 0.2194\n",
      "Epoch 42/2000\n",
      "182/182 - 1s - loss: 0.0930 - val_loss: 0.2161\n",
      "Epoch 43/2000\n",
      "182/182 - 1s - loss: 0.0898 - val_loss: 0.2168\n",
      "Epoch 44/2000\n",
      "182/182 - 1s - loss: 0.0813 - val_loss: 0.2233\n",
      "Epoch 45/2000\n",
      "182/182 - 1s - loss: 0.0735 - val_loss: 0.2192\n",
      "Epoch 46/2000\n",
      "182/182 - 1s - loss: 0.0779 - val_loss: 0.2169\n",
      "Epoch 47/2000\n",
      "182/182 - 1s - loss: 0.0746 - val_loss: 0.2219\n",
      "Epoch 48/2000\n",
      "182/182 - 1s - loss: 0.0663 - val_loss: 0.2296\n",
      "Epoch 49/2000\n",
      "182/182 - 1s - loss: 0.0618 - val_loss: 0.2254\n",
      "Epoch 50/2000\n",
      "182/182 - 1s - loss: 0.0644 - val_loss: 0.2244\n",
      "Epoch 51/2000\n",
      "182/182 - 1s - loss: 0.0556 - val_loss: 0.2283\n",
      "Epoch 52/2000\n",
      "182/182 - 1s - loss: 0.0582 - val_loss: 0.2356\n",
      "Epoch 53/2000\n",
      "182/182 - 1s - loss: 0.0522 - val_loss: 0.2335\n",
      "Epoch 54/2000\n",
      "182/182 - 1s - loss: 0.0513 - val_loss: 0.2386\n",
      "Epoch 55/2000\n",
      "182/182 - 2s - loss: 0.0468 - val_loss: 0.2374\n",
      "Epoch 56/2000\n",
      "182/182 - 2s - loss: 0.0463 - val_loss: 0.2431\n",
      "Epoch 57/2000\n",
      "182/182 - 1s - loss: 0.0455 - val_loss: 0.2480\n",
      "Epoch 58/2000\n",
      "182/182 - 1s - loss: 0.0456 - val_loss: 0.2464\n",
      "Epoch 59/2000\n",
      "182/182 - 1s - loss: 0.0453 - val_loss: 0.2386\n",
      "Epoch 60/2000\n",
      "182/182 - 1s - loss: 0.0432 - val_loss: 0.2432\n",
      "Epoch 61/2000\n",
      "182/182 - 1s - loss: 0.0371 - val_loss: 0.2518\n",
      "Epoch 62/2000\n",
      "182/182 - 1s - loss: 0.0392 - val_loss: 0.2370\n",
      "Epoch 63/2000\n",
      "182/182 - 1s - loss: 0.0418 - val_loss: 0.2384\n",
      "Epoch 64/2000\n",
      "182/182 - 1s - loss: 0.0252 - val_loss: 0.2517\n",
      "Epoch 65/2000\n",
      "182/182 - 1s - loss: 0.0335 - val_loss: 0.2541\n",
      "Epoch 66/2000\n",
      "182/182 - 1s - loss: 0.0335 - val_loss: 0.2538\n",
      "Evaluation:\n",
      "\n",
      "MSE: 0.253844742823248\n",
      "AUC: 0.7126654064272212\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Beginning cross validation for candidate number 1\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Total size of the training set: 182\n",
      "Total size of the validation set: 46\n",
      "Class frequencies in the training set: [0.5 0.5]\n",
      "Class frequencies in the validation set: [0.5 0.5]\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Beginning the tree building procedure\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Fitting candidate number 2 in shuffle 1 with parameters\n",
      "\n",
      "{'activation': 'elu', 'batch_size': 64, 'callbacks': [<tensorflow.python.keras.callbacks.EarlyStopping object at 0x000002312EDC0508>], 'dropout': 0.3, 'epochs': 2000, 'filters': 100, 'input_shape': None, 'kernel_size': (3, 5), 'learning_rate': 0.001, 'loss': 'mse', 'n_classes': 2, 'n_layers': 2, 'pool_size': (2, 2), 'verbose': 2}\n",
      "Train on 182 samples, validate on 46 samples\n",
      "Epoch 1/2000\n",
      "182/182 - 2s - loss: 0.2502 - val_loss: 0.2493\n",
      "Epoch 2/2000\n",
      "182/182 - 1s - loss: 0.2472 - val_loss: 0.2495\n",
      "Epoch 3/2000\n",
      "182/182 - 1s - loss: 0.2465 - val_loss: 0.2497\n",
      "Epoch 4/2000\n",
      "182/182 - 1s - loss: 0.2459 - val_loss: 0.2493\n",
      "Epoch 5/2000\n",
      "182/182 - 1s - loss: 0.2406 - val_loss: 0.2487\n",
      "Epoch 6/2000\n",
      "182/182 - 1s - loss: 0.2385 - val_loss: 0.2481\n",
      "Epoch 7/2000\n",
      "182/182 - 1s - loss: 0.2344 - val_loss: 0.2470\n",
      "Epoch 8/2000\n",
      "182/182 - 1s - loss: 0.2293 - val_loss: 0.2468\n",
      "Epoch 9/2000\n",
      "182/182 - 1s - loss: 0.2333 - val_loss: 0.2459\n",
      "Epoch 10/2000\n",
      "182/182 - 1s - loss: 0.2227 - val_loss: 0.2470\n",
      "Epoch 11/2000\n",
      "182/182 - 1s - loss: 0.2194 - val_loss: 0.2443\n",
      "Epoch 12/2000\n",
      "182/182 - 1s - loss: 0.2137 - val_loss: 0.2435\n",
      "Epoch 13/2000\n",
      "182/182 - 1s - loss: 0.2061 - val_loss: 0.2430\n",
      "Epoch 14/2000\n",
      "182/182 - 1s - loss: 0.2049 - val_loss: 0.2435\n",
      "Epoch 15/2000\n",
      "182/182 - 1s - loss: 0.1953 - val_loss: 0.2429\n",
      "Epoch 16/2000\n",
      "182/182 - 1s - loss: 0.1984 - val_loss: 0.2446\n",
      "Epoch 17/2000\n",
      "182/182 - 1s - loss: 0.1975 - val_loss: 0.2426\n",
      "Epoch 18/2000\n",
      "182/182 - 1s - loss: 0.1855 - val_loss: 0.2430\n",
      "Epoch 19/2000\n",
      "182/182 - 1s - loss: 0.1870 - val_loss: 0.2445\n",
      "Epoch 20/2000\n",
      "182/182 - 1s - loss: 0.1710 - val_loss: 0.2457\n",
      "Epoch 21/2000\n",
      "182/182 - 1s - loss: 0.1791 - val_loss: 0.2485\n",
      "Epoch 22/2000\n",
      "182/182 - 1s - loss: 0.1666 - val_loss: 0.2475\n",
      "Epoch 23/2000\n",
      "182/182 - 1s - loss: 0.1626 - val_loss: 0.2489\n",
      "Epoch 24/2000\n",
      "182/182 - 1s - loss: 0.1485 - val_loss: 0.2508\n",
      "Epoch 25/2000\n",
      "182/182 - 1s - loss: 0.1533 - val_loss: 0.2570\n",
      "Epoch 26/2000\n",
      "182/182 - 1s - loss: 0.1480 - val_loss: 0.2540\n",
      "Epoch 27/2000\n",
      "182/182 - 1s - loss: 0.1422 - val_loss: 0.2545\n",
      "Epoch 28/2000\n",
      "182/182 - 1s - loss: 0.1428 - val_loss: 0.2548\n",
      "Epoch 29/2000\n",
      "182/182 - 1s - loss: 0.1270 - val_loss: 0.2604\n",
      "Epoch 30/2000\n",
      "182/182 - 1s - loss: 0.1202 - val_loss: 0.2630\n",
      "Epoch 31/2000\n",
      "182/182 - 1s - loss: 0.1224 - val_loss: 0.2649\n",
      "Epoch 32/2000\n",
      "182/182 - 1s - loss: 0.1245 - val_loss: 0.2644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/2000\n",
      "182/182 - 1s - loss: 0.1160 - val_loss: 0.2634\n",
      "Epoch 34/2000\n",
      "182/182 - 1s - loss: 0.1118 - val_loss: 0.2642\n",
      "Epoch 35/2000\n",
      "182/182 - 1s - loss: 0.1074 - val_loss: 0.2678\n",
      "Epoch 36/2000\n",
      "182/182 - 1s - loss: 0.0967 - val_loss: 0.2748\n",
      "Epoch 37/2000\n",
      "182/182 - 1s - loss: 0.1019 - val_loss: 0.2779\n",
      "Epoch 38/2000\n",
      "182/182 - 1s - loss: 0.0873 - val_loss: 0.2790\n",
      "Epoch 39/2000\n",
      "182/182 - 1s - loss: 0.0805 - val_loss: 0.2811\n",
      "Epoch 40/2000\n",
      "182/182 - 1s - loss: 0.0725 - val_loss: 0.2826\n",
      "Epoch 41/2000\n",
      "182/182 - 1s - loss: 0.0849 - val_loss: 0.2818\n",
      "Epoch 42/2000\n",
      "182/182 - 1s - loss: 0.0739 - val_loss: 0.2825\n",
      "Epoch 43/2000\n",
      "182/182 - 1s - loss: 0.0676 - val_loss: 0.2845\n",
      "Epoch 44/2000\n",
      "182/182 - 1s - loss: 0.0636 - val_loss: 0.2855\n",
      "Epoch 45/2000\n",
      "182/182 - 1s - loss: 0.0640 - val_loss: 0.2890\n",
      "Epoch 46/2000\n",
      "182/182 - 1s - loss: 0.0544 - val_loss: 0.2932\n",
      "Epoch 47/2000\n",
      "182/182 - 1s - loss: 0.0571 - val_loss: 0.2950\n",
      "Epoch 48/2000\n",
      "182/182 - 1s - loss: 0.0509 - val_loss: 0.2962\n",
      "Epoch 49/2000\n",
      "182/182 - 1s - loss: 0.0547 - val_loss: 0.2965\n",
      "Epoch 50/2000\n",
      "182/182 - 1s - loss: 0.0470 - val_loss: 0.2980\n",
      "Epoch 51/2000\n",
      "182/182 - 1s - loss: 0.0478 - val_loss: 0.2996\n",
      "Epoch 52/2000\n",
      "182/182 - 1s - loss: 0.0404 - val_loss: 0.3009\n",
      "Epoch 53/2000\n",
      "182/182 - 1s - loss: 0.0440 - val_loss: 0.3021\n",
      "Epoch 54/2000\n",
      "182/182 - 1s - loss: 0.0402 - val_loss: 0.3030\n",
      "Epoch 55/2000\n",
      "182/182 - 1s - loss: 0.0401 - val_loss: 0.3040\n",
      "Epoch 56/2000\n",
      "182/182 - 2s - loss: 0.0400 - val_loss: 0.3041\n",
      "Epoch 57/2000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-0ac55481777a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[1;31m# Seting model parameters and fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[1;31m# Evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    407\u001b[0m           \u001b[0mvalidation_in_fit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m           \u001b[0mprepared_feed_values_from_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m           steps_name='validation_steps')\n\u001b[0m\u001b[0;32m    410\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_results\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[0mval_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mval_results\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "skf=StratifiedKFold(n_splits = n_cv_folds)\n",
    "\n",
    "param_grid = {\n",
    "    'input_shape': [None],\n",
    "    'n_layers': [2],\n",
    "    'filters': [100],\n",
    "    'kernel_size': [(3, 5)],\n",
    "    'pool_size': [(2, 2)],\n",
    "    'activation': ['elu'],\n",
    "    'n_classes': [2],\n",
    "    'learning_rate': [0.001],\n",
    "    'loss': ['mse'],\n",
    "    'dropout': [0.3],\n",
    "    'batch_size': [64],\n",
    "    'epochs': [2000],\n",
    "    'callbacks': [[EarlyStopping(patience=40)]],\n",
    "    'verbose': [2]\n",
    "}\n",
    "\n",
    "grid_size = 1\n",
    "for key, value in param_grid.items():\n",
    "    grid_size *= len(value)\n",
    "fit_keys = ['batch_size', 'epochs', 'callbacks', 'verbose']\n",
    "\n",
    "###############################################################\n",
    "# Starting stability selection loop\n",
    "###############################################################\n",
    "test_stat_df = pd.DataFrame(index=[\"AUC\", \"Weighted MSE\", \"Params\", \"Features\"], columns=[i+1 for i in range(n_shuffles)])\n",
    "shuffle_counter = 0\n",
    "\n",
    "print(X.shape)\n",
    "print(type(X))\n",
    "\n",
    "n_values = np.max(y) + 1\n",
    "labels_oh = np.eye(n_values)[y]\n",
    "\n",
    "for samples,test in StratShufSpl.split(X, y):\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"Beginning stability selection iteration {}\".format(shuffle_counter))\n",
    "    print(\"----------------------------------------------------------------\\n\")\n",
    "    shuffle_counter+=1\n",
    "    X_train, X_test = X.iloc[samples], X.iloc[test]\n",
    "    y_train, y_test=y[samples], y[test]\n",
    "\n",
    "    # Creating a dataframe for storing the results\n",
    "    cv_list = [\"CV_{}_GS_{}\".format(str(i+1), str(j+1)) for i in range(n_cv_folds) for j in range(len(ParameterGrid(param_grid)))]\n",
    "    stat_df = pd.DataFrame(index=[\"AUC\", \"Weighted MSE\", \"Params\", \"Features\"], columns=cv_list)\n",
    "\n",
    "    # Performing the grid search CV\n",
    "    n_candidates = n_cv_folds * grid_size\n",
    "    cv_fold = 0\n",
    "    candidate_counter = 0\n",
    "    print('Performing GridSearchCV for {} candidates\\n\\n'.format(n_candidates))\n",
    "    for train_index, test_index in skf.split(X_train, y_train):\n",
    "        print(\"----------------------------------------------------------------\")\n",
    "        print(\"Beginning cross validation for candidate number {}\".format(candidate_counter))\n",
    "        print(\"----------------------------------------------------------------\\n\")\n",
    "        #################################################################\n",
    "        # Select and format training and testing sets\n",
    "        #################################################################\n",
    "        cv_fold += 1\n",
    "        gs_it = 0\n",
    "\n",
    "        train_X, val_X = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        train_index = samples[train_index]\n",
    "        test_index = samples[test_index]\n",
    "        train_y, val_y = labels_oh[train_index,:], labels_oh[test_index,:]\n",
    "        class_frequencies_train = np.sum(train_y, axis = 0)/len(train_y)\n",
    "        class_frequencies_val = np.sum(val_y, axis = 0)/len(val_y)\n",
    "        print('Total size of the training set: {}'.format(len(train_X)))\n",
    "        print('Total size of the validation set: {}'.format(len(val_X)))\n",
    "        print('Class frequencies in the training set: {}'.format(class_frequencies_train))\n",
    "        print('Class frequencies in the validation set: {}\\n'.format(class_frequencies_val))\n",
    "     \n",
    "        \n",
    "        # Build tree\n",
    "        print(\"----------------------------------------------------------------\")\n",
    "        print(\"Beginning the tree building procedure\")\n",
    "        print(\"----------------------------------------------------------------\\n\")\n",
    "\n",
    "        tree_builder = TreeBuilder(tree_path)\n",
    "        tree_builder = tree_builder.fit(train_X, train_y)\n",
    "        train_X = tree_builder.transform(train_X)\n",
    "        val_X = tree_builder.transform(val_X) \n",
    "\n",
    "        for g in ParameterGrid(param_grid):\n",
    "            candidate_counter += 1\n",
    "            print('Fitting candidate number {} in shuffle {} with parameters\\n'.format(candidate_counter, shuffle_counter))\n",
    "            print(g)\n",
    "            gs_it += 1\n",
    "            params = g.copy()\n",
    "            fit_params = {key: g.pop(key) for key in fit_keys}\n",
    "\n",
    "            num_train_samples = train_X.shape[0]\n",
    "            num_test_samples = val_X.shape[0]\n",
    "            tree_row = train_X.shape[1]\n",
    "            tree_col = train_X.shape[2]\n",
    "\n",
    "            g['input_shape'] = (tree_row, tree_col)\n",
    "        \n",
    "            fit_params['x'] = train_X\n",
    "            fit_params['y'] = train_y\n",
    "            fit_params['validation_data'] = (val_X, val_y)\n",
    "            \n",
    "            # Seting model parameters and fitting\n",
    "            model = build_model(**g)\n",
    "            model.fit(**fit_params)\n",
    "\n",
    "            # Evaluation\n",
    "            print('Evaluation:\\n')\n",
    "            val_preds = model.predict(val_X)\n",
    "            auc_score = roc_auc_score(val_y, val_preds)\n",
    "            mse = mean_squared_error(val_y, val_preds)\n",
    "            #mse, auc = model.evaluate(x = val_X, y = val_y, verbose = 0)\n",
    "            print('MSE: {}'.format(mse))\n",
    "            print('AUC: {}\\n'.format(auc_score))\n",
    "            \n",
    "            # Storing stats in dataframe\n",
    "            stat_df.loc[\"Weighted MSE\"][\"CV_{}_GS_{}\".format(cv_fold, gs_it)] = mse\n",
    "            stat_df.loc[\"AUC\"][\"CV_{}_GS_{}\".format(cv_fold, gs_it)] = auc_score\n",
    "            stat_df.loc[\"Params\"][\"CV_{}_GS_{}\".format(cv_fold, gs_it)] = params\n",
    "            stat_df.loc[\"Features\"][\"CV_{}_GS_{}\".format(cv_fold, gs_it)] = tree_builder.features\n",
    "\n",
    "            # Resetting model weights and clearing the session\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "    print(stat_df)\n",
    "    # End of GS_CV\n",
    "    # Find best params according to lowest weighted MSE\n",
    "    # Refit model on train + val with best params\n",
    "    # Report all scores on test\n",
    "    # Save in test_stat_df\n",
    "\n",
    "    # Saving the results\n",
    "    try:\n",
    "        os.mkdir('output_data')\n",
    "    except OSError as error:\n",
    "        print('Directory already exists')\n",
    "\n",
    "    stat_df.to_csv('output_data/validation_results_{}.csv'.format(shuffle_counter))\n",
    "\n",
    "\n",
    "    # Reftting on the entire training set with the best found parameters\n",
    "    print('Reftting on the entire training set with the best found parameters\\n')\n",
    "    tf.keras.backend.clear_session()\n",
    "    grouped_df, params = group_by_params(stat_df, num_combinations = grid_size)\n",
    "    grouped_df.to_csv('output_data/grouped_validation_results_{}.csv'.format(shuffle_counter))\n",
    "    best_score_index = np.argmin(list(grouped_df.loc['Weighted MSE']))\n",
    "    best_params = params[best_score_index]\n",
    "    print('Best found parameters:\\n')\n",
    "    print(best_params)\n",
    "    X_train = np.log(X_train + 1)\n",
    "    X_test = np.log(X_test + 1)\n",
    "    y_train, y_test = labels_oh[samples,:], labels_oh[test,:]\n",
    "\n",
    "    \n",
    "    # Build tree\n",
    "    tree_builder = TreeBuilder(tree_path)\n",
    "    tree_builder = tree_builder.fit(X_train, y_train)\n",
    "    X_train = tree_builder.transform(X_train)\n",
    "    X_test = tree_builder.transform(X_test) \n",
    "\n",
    "    num_train_samples = X_train.shape[0]\n",
    "    num_test_samples = X_test.shape[0]\n",
    "    tree_row = X_train.shape[1]\n",
    "    tree_col = X_train.shape[2]\n",
    "\n",
    "    fit_params = {key: best_params.pop(key) for key in fit_keys}\n",
    "\n",
    "    # Splitting the data in train and val for early stopping\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "                                                    stratify=y_train, \n",
    "                                                   test_size=0.1)\n",
    "    fit_params['x'] = X_train\n",
    "    fit_params['y'] = y_train\n",
    "    fit_params['validation_data'] = (X_val, y_val)\n",
    "    best_params['input_shape'] = (tree_row, tree_col)\n",
    "\n",
    "    print('Samples X_train: {}'.format(len(X_train)))\n",
    "    print('Samples X_test: {}'.format(len(X_test)))\n",
    "    print('Labels in y_train: {}\\n'.format(np.sum(y_train, axis = 0)))\n",
    "\n",
    "    #test_untrained_weights = model.get_weights().copy()\n",
    "    model = build_model(**best_params)\n",
    "    print('Rebuilt model and thus reinitialized')\n",
    "    model.fit(**fit_params)\n",
    "    \n",
    "    y_pred_test = model.predict(X_test)[:, 0]\n",
    "    y_test = y_test[:, 0]\n",
    "    np.save(\"output_data/y_test_{}.npy\".format(shuffle_counter), y_test)\n",
    "    np.save(\"output_data/y_pred_test_{}.npy\".format(shuffle_counter), y_pred_test)\n",
    "\n",
    "    ### ONLY FOR A TEST\n",
    "    #np.save(\"X_train.npy\", X_train)\n",
    "    #model.save(\"test_model\")\n",
    "\n",
    "    final_params = {**best_params, **fit_params}\n",
    "    del final_params['x']\n",
    "    del final_params['y']\n",
    "    del final_params['validation_data']\n",
    "\n",
    "    mse = model.evaluate(x = X_test, y = y_test, verbose = 0)\n",
    "    auc_score = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "    test_stat_df.loc[\"Weighted MSE\"][shuffle_counter] = mse\n",
    "    test_stat_df.loc[\"AUC\"][shuffle_counter] = auc_score\n",
    "    test_stat_df.loc[\"Features\"][shuffle_counter] = tree_builder.features\n",
    "    test_stat_df.loc[\"Params\"][shuffle_counter] = final_params\n",
    "\n",
    "    # Storing the FPR, TPR and thresholds for creating an AUC plot\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_test, pos_label=1)\n",
    "    auc_dict[\"FPR\"].append(fpr)\n",
    "    auc_dict[\"TPR\"].append(tpr)\n",
    "    auc_dict[\"Thresholds\"].append(thresholds)\n",
    "    auc_dict[\"AUC\"].append(auc_score)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test,y_pred_test,pos_label=1)\n",
    "    auc_roc1 = auc(fpr, tpr)\n",
    "    list_auc.append(auc_roc1)\n",
    "    \n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    aucs.append(auc_roc1)\n",
    "\n",
    "\n",
    "    # Resetting model weights and clearing the session\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=1, color='r',\n",
    "         label='Random guessing', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=1, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color = 'grey', alpha = .2,\n",
    "                 label = r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.savefig('output_data/auc_avg.pdf', bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
