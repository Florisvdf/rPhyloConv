{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import struct\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from numpy import unique\n",
    "import pandas as pd\n",
    "from array import array as pyarray\n",
    "from scipy import sparse, interp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, mean_absolute_error,accuracy_score, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, StratifiedShuffleSplit, ShuffleSplit, GridSearchCV, ParameterGrid, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, matthews_corrcoef, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin\n",
    "\n",
    "from Bio import Phylo\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from copy import deepcopy\n",
    "\n",
    "from models import build_model\n",
    "from utils.io import group_by_params\n",
    "from utils.treebuilding import TreeBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(256)\n",
    "random_seed = 42\n",
    "n_shuffles = 1\n",
    "test_data_ratio=0.2\n",
    "n_cv_folds = 5\n",
    "train_ratio = 1\n",
    "\n",
    "tree_path = \"tree.tree\" \n",
    "\n",
    "path = str(np.loadtxt(\"path.txt\", dtype = str))\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"output_data\")\n",
    "except OSError:\n",
    "    print(\"Directory already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"{}/X.csv\".format(path), index_col = 0)\n",
    "y = np.load(\"{}/y.npy\".format(path)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np = np.load(\"{}/X.npy\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = {0:'aTPO Negative', 1:'aTPO Positive'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_dict = {}\n",
    "auc_dict[\"TPR\"] = []\n",
    "auc_dict[\"FPR\"] = []\n",
    "auc_dict[\"Thresholds\"] = []\n",
    "auc_dict[\"AUC\"] = []\n",
    "\n",
    "list_auc=[]\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "shuffle_counter = 0\n",
    "\n",
    "StratShufSpl = StratifiedShuffleSplit(n_shuffles,\n",
    "                                    test_size = test_data_ratio, \n",
    "                                    random_state = random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(286, 449)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "----------------------------------------------------------------\n",
      "Beginning stability selection iteration 0\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Performing GridSearchCV for 5 candidates\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Beginning cross validation for candidate number 0\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Total size of the training set: 182\n",
      "Total size of the validation set: 46\n",
      "Class frequencies in the training set: [0.5 0.5]\n",
      "Class frequencies in the validation set: [0.5 0.5]\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Beginning the tree building procedure\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Fitting candidate number 1 in shuffle 1 with parameters\n",
      "\n",
      "{'activation': 'elu', 'batch_size': 64, 'callbacks': [<tensorflow.python.keras.callbacks.EarlyStopping object at 0x0000019C07A8A6C8>], 'dropout': 0.3, 'epochs': 2000, 'filters': 10, 'input_shape': None, 'kernel_size': (3, 9), 'learning_rate': 0.001, 'loss': 'mse', 'n_classes': 2, 'n_layers': 1, 'pool_size': (2, 2), 'verbose': 2}\n",
      "Train on 182 samples, validate on 46 samples\n",
      "Epoch 1/2000\n",
      "182/182 - 1s - loss: 0.2481 - val_loss: 0.2454\n",
      "Epoch 2/2000\n",
      "182/182 - 0s - loss: 0.2305 - val_loss: 0.2445\n",
      "Epoch 3/2000\n",
      "182/182 - 0s - loss: 0.2298 - val_loss: 0.2410\n",
      "Epoch 4/2000\n",
      "182/182 - 0s - loss: 0.2162 - val_loss: 0.2382\n",
      "Epoch 5/2000\n",
      "182/182 - 0s - loss: 0.2048 - val_loss: 0.2351\n",
      "Epoch 6/2000\n",
      "182/182 - 0s - loss: 0.1996 - val_loss: 0.2305\n",
      "Epoch 7/2000\n",
      "182/182 - 0s - loss: 0.1835 - val_loss: 0.2258\n",
      "Epoch 8/2000\n",
      "182/182 - 0s - loss: 0.1781 - val_loss: 0.2216\n",
      "Epoch 9/2000\n",
      "182/182 - 0s - loss: 0.1673 - val_loss: 0.2173\n",
      "Epoch 10/2000\n",
      "182/182 - 0s - loss: 0.1538 - val_loss: 0.2134\n",
      "Epoch 11/2000\n",
      "182/182 - 0s - loss: 0.1463 - val_loss: 0.2100\n",
      "Epoch 12/2000\n",
      "182/182 - 0s - loss: 0.1375 - val_loss: 0.2066\n",
      "Epoch 13/2000\n",
      "182/182 - 0s - loss: 0.1256 - val_loss: 0.2035\n",
      "Epoch 14/2000\n",
      "182/182 - 0s - loss: 0.1252 - val_loss: 0.2002\n",
      "Epoch 15/2000\n",
      "182/182 - 0s - loss: 0.1137 - val_loss: 0.1969\n",
      "Epoch 16/2000\n",
      "182/182 - 0s - loss: 0.1070 - val_loss: 0.1941\n",
      "Epoch 17/2000\n",
      "182/182 - 0s - loss: 0.0921 - val_loss: 0.1917\n",
      "Epoch 18/2000\n",
      "182/182 - 0s - loss: 0.0903 - val_loss: 0.1893\n",
      "Epoch 19/2000\n",
      "182/182 - 0s - loss: 0.0877 - val_loss: 0.1875\n",
      "Epoch 20/2000\n",
      "182/182 - 0s - loss: 0.0781 - val_loss: 0.1836\n",
      "Epoch 21/2000\n",
      "182/182 - 0s - loss: 0.0704 - val_loss: 0.1809\n",
      "Epoch 22/2000\n",
      "182/182 - 0s - loss: 0.0636 - val_loss: 0.1787\n",
      "Epoch 23/2000\n",
      "182/182 - 0s - loss: 0.0569 - val_loss: 0.1759\n",
      "Epoch 24/2000\n",
      "182/182 - 0s - loss: 0.0598 - val_loss: 0.1728\n",
      "Epoch 25/2000\n",
      "182/182 - 0s - loss: 0.0506 - val_loss: 0.1704\n",
      "Epoch 26/2000\n",
      "182/182 - 0s - loss: 0.0481 - val_loss: 0.1682\n",
      "Epoch 27/2000\n",
      "182/182 - 0s - loss: 0.0451 - val_loss: 0.1661\n",
      "Epoch 28/2000\n",
      "182/182 - 0s - loss: 0.0440 - val_loss: 0.1634\n",
      "Epoch 29/2000\n",
      "182/182 - 0s - loss: 0.0365 - val_loss: 0.1617\n",
      "Epoch 30/2000\n",
      "182/182 - 0s - loss: 0.0326 - val_loss: 0.1607\n",
      "Epoch 31/2000\n",
      "182/182 - 0s - loss: 0.0315 - val_loss: 0.1596\n",
      "Epoch 32/2000\n",
      "182/182 - 0s - loss: 0.0307 - val_loss: 0.1579\n",
      "Epoch 33/2000\n",
      "182/182 - 0s - loss: 0.0302 - val_loss: 0.1569\n",
      "Epoch 34/2000\n",
      "182/182 - 0s - loss: 0.0269 - val_loss: 0.1560\n",
      "Epoch 35/2000\n",
      "182/182 - 0s - loss: 0.0231 - val_loss: 0.1548\n",
      "Epoch 36/2000\n",
      "182/182 - 0s - loss: 0.0225 - val_loss: 0.1535\n",
      "Epoch 37/2000\n",
      "182/182 - 0s - loss: 0.0233 - val_loss: 0.1533\n",
      "Epoch 38/2000\n",
      "182/182 - 0s - loss: 0.0174 - val_loss: 0.1528\n",
      "Epoch 39/2000\n",
      "182/182 - 0s - loss: 0.0184 - val_loss: 0.1529\n",
      "Epoch 40/2000\n",
      "182/182 - 0s - loss: 0.0188 - val_loss: 0.1527\n",
      "Epoch 41/2000\n",
      "182/182 - 0s - loss: 0.0169 - val_loss: 0.1525\n",
      "Epoch 42/2000\n",
      "182/182 - 0s - loss: 0.0155 - val_loss: 0.1525\n",
      "Epoch 43/2000\n",
      "182/182 - 0s - loss: 0.0129 - val_loss: 0.1526\n",
      "Epoch 44/2000\n",
      "182/182 - 0s - loss: 0.0138 - val_loss: 0.1523\n",
      "Epoch 45/2000\n",
      "182/182 - 0s - loss: 0.0117 - val_loss: 0.1521\n",
      "Epoch 46/2000\n",
      "182/182 - 0s - loss: 0.0133 - val_loss: 0.1514\n",
      "Epoch 47/2000\n",
      "182/182 - 0s - loss: 0.0116 - val_loss: 0.1512\n",
      "Epoch 48/2000\n",
      "182/182 - 0s - loss: 0.0119 - val_loss: 0.1509\n",
      "Epoch 49/2000\n",
      "182/182 - 0s - loss: 0.0130 - val_loss: 0.1505\n",
      "Epoch 50/2000\n",
      "182/182 - 0s - loss: 0.0105 - val_loss: 0.1508\n",
      "Epoch 51/2000\n",
      "182/182 - 0s - loss: 0.0110 - val_loss: 0.1498\n",
      "Epoch 52/2000\n",
      "182/182 - 0s - loss: 0.0088 - val_loss: 0.1496\n",
      "Epoch 53/2000\n",
      "182/182 - 0s - loss: 0.0075 - val_loss: 0.1497\n",
      "Epoch 54/2000\n",
      "182/182 - 0s - loss: 0.0093 - val_loss: 0.1498\n",
      "Epoch 55/2000\n",
      "182/182 - 0s - loss: 0.0075 - val_loss: 0.1499\n",
      "Epoch 56/2000\n",
      "182/182 - 0s - loss: 0.0083 - val_loss: 0.1486\n",
      "Epoch 57/2000\n",
      "182/182 - 0s - loss: 0.0074 - val_loss: 0.1485\n",
      "Epoch 58/2000\n",
      "182/182 - 0s - loss: 0.0067 - val_loss: 0.1494\n",
      "Epoch 59/2000\n",
      "182/182 - 0s - loss: 0.0077 - val_loss: 0.1512\n",
      "Epoch 60/2000\n",
      "182/182 - 0s - loss: 0.0056 - val_loss: 0.1482\n",
      "Epoch 61/2000\n",
      "182/182 - 0s - loss: 0.0058 - val_loss: 0.1480\n",
      "Epoch 62/2000\n",
      "182/182 - 0s - loss: 0.0061 - val_loss: 0.1485\n",
      "Epoch 63/2000\n",
      "182/182 - 0s - loss: 0.0067 - val_loss: 0.1496\n",
      "Epoch 64/2000\n",
      "182/182 - 0s - loss: 0.0078 - val_loss: 0.1509\n",
      "Epoch 65/2000\n",
      "182/182 - 0s - loss: 0.0058 - val_loss: 0.1498\n",
      "Epoch 66/2000\n",
      "182/182 - 0s - loss: 0.0062 - val_loss: 0.1497\n",
      "Epoch 67/2000\n",
      "182/182 - 0s - loss: 0.0048 - val_loss: 0.1499\n",
      "Epoch 68/2000\n",
      "182/182 - 0s - loss: 0.0052 - val_loss: 0.1500\n",
      "Epoch 69/2000\n",
      "182/182 - 0s - loss: 0.0053 - val_loss: 0.1495\n",
      "Epoch 70/2000\n",
      "182/182 - 0s - loss: 0.0047 - val_loss: 0.1493\n",
      "Epoch 71/2000\n",
      "182/182 - 0s - loss: 0.0048 - val_loss: 0.1494\n",
      "Epoch 72/2000\n",
      "182/182 - 0s - loss: 0.0044 - val_loss: 0.1505\n",
      "Epoch 73/2000\n",
      "182/182 - 0s - loss: 0.0038 - val_loss: 0.1513\n",
      "Epoch 74/2000\n",
      "182/182 - 0s - loss: 0.0042 - val_loss: 0.1494\n",
      "Epoch 75/2000\n",
      "182/182 - 0s - loss: 0.0031 - val_loss: 0.1488\n",
      "Epoch 76/2000\n",
      "182/182 - 0s - loss: 0.0034 - val_loss: 0.1489\n",
      "Epoch 77/2000\n",
      "182/182 - 0s - loss: 0.0032 - val_loss: 0.1501\n",
      "Epoch 78/2000\n",
      "182/182 - 0s - loss: 0.0036 - val_loss: 0.1517\n",
      "Epoch 79/2000\n",
      "182/182 - 0s - loss: 0.0035 - val_loss: 0.1509\n",
      "Epoch 80/2000\n",
      "182/182 - 0s - loss: 0.0036 - val_loss: 0.1498\n",
      "Epoch 81/2000\n",
      "182/182 - 0s - loss: 0.0030 - val_loss: 0.1497\n",
      "Epoch 82/2000\n",
      "182/182 - 0s - loss: 0.0032 - val_loss: 0.1499\n",
      "Epoch 83/2000\n",
      "182/182 - 0s - loss: 0.0029 - val_loss: 0.1508\n",
      "Epoch 84/2000\n",
      "182/182 - 0s - loss: 0.0038 - val_loss: 0.1521\n",
      "Epoch 85/2000\n",
      "182/182 - 0s - loss: 0.0027 - val_loss: 0.1517\n",
      "Epoch 86/2000\n",
      "182/182 - 0s - loss: 0.0024 - val_loss: 0.1511\n",
      "Epoch 87/2000\n",
      "182/182 - 0s - loss: 0.0023 - val_loss: 0.1511\n",
      "Epoch 88/2000\n",
      "182/182 - 0s - loss: 0.0032 - val_loss: 0.1518\n",
      "Epoch 89/2000\n",
      "182/182 - 0s - loss: 0.0025 - val_loss: 0.1525\n",
      "Epoch 90/2000\n",
      "182/182 - 0s - loss: 0.0030 - val_loss: 0.1546\n",
      "Epoch 91/2000\n",
      "182/182 - 0s - loss: 0.0031 - val_loss: 0.1544\n",
      "Epoch 92/2000\n",
      "182/182 - 0s - loss: 0.0021 - val_loss: 0.1542\n",
      "Epoch 93/2000\n",
      "182/182 - 0s - loss: 0.0026 - val_loss: 0.1514\n",
      "Epoch 94/2000\n",
      "182/182 - 0s - loss: 0.0027 - val_loss: 0.1511\n",
      "Epoch 95/2000\n",
      "182/182 - 0s - loss: 0.0021 - val_loss: 0.1511\n",
      "Epoch 96/2000\n",
      "182/182 - 0s - loss: 0.0024 - val_loss: 0.1527\n",
      "Epoch 97/2000\n",
      "182/182 - 0s - loss: 0.0020 - val_loss: 0.1568\n",
      "Epoch 98/2000\n",
      "182/182 - 0s - loss: 0.0023 - val_loss: 0.1574\n",
      "Epoch 99/2000\n",
      "182/182 - 0s - loss: 0.0023 - val_loss: 0.1528\n",
      "Epoch 100/2000\n",
      "182/182 - 0s - loss: 0.0020 - val_loss: 0.1519\n",
      "Epoch 101/2000\n",
      "182/182 - 0s - loss: 0.0016 - val_loss: 0.1522\n",
      "Evaluation:\n",
      "\n",
      "MSE: 0.15217033494194382\n",
      "AUC: 0.8525519848771266\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Beginning cross validation for candidate number 1\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Total size of the training set: 182\n",
      "Total size of the validation set: 46\n",
      "Class frequencies in the training set: [0.5 0.5]\n",
      "Class frequencies in the validation set: [0.5 0.5]\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Beginning the tree building procedure\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Fitting candidate number 2 in shuffle 1 with parameters\n",
      "\n",
      "{'activation': 'elu', 'batch_size': 64, 'callbacks': [<tensorflow.python.keras.callbacks.EarlyStopping object at 0x0000019C07A8A6C8>], 'dropout': 0.3, 'epochs': 2000, 'filters': 10, 'input_shape': None, 'kernel_size': (3, 9), 'learning_rate': 0.001, 'loss': 'mse', 'n_classes': 2, 'n_layers': 1, 'pool_size': (2, 2), 'verbose': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 182 samples, validate on 46 samples\n",
      "Epoch 1/2000\n",
      "182/182 - 1s - loss: 0.2521 - val_loss: 0.2504\n",
      "Epoch 2/2000\n",
      "182/182 - 0s - loss: 0.2460 - val_loss: 0.2488\n",
      "Epoch 3/2000\n",
      "182/182 - 0s - loss: 0.2428 - val_loss: 0.2469\n",
      "Epoch 4/2000\n",
      "182/182 - 0s - loss: 0.2401 - val_loss: 0.2447\n",
      "Epoch 5/2000\n",
      "182/182 - 0s - loss: 0.2362 - val_loss: 0.2423\n",
      "Epoch 6/2000\n",
      "182/182 - 0s - loss: 0.2289 - val_loss: 0.2400\n",
      "Epoch 7/2000\n",
      "182/182 - 0s - loss: 0.2211 - val_loss: 0.2374\n",
      "Epoch 8/2000\n",
      "182/182 - 0s - loss: 0.2161 - val_loss: 0.2346\n",
      "Epoch 9/2000\n",
      "182/182 - 0s - loss: 0.2108 - val_loss: 0.2316\n",
      "Epoch 10/2000\n",
      "182/182 - 0s - loss: 0.1985 - val_loss: 0.2287\n",
      "Epoch 11/2000\n",
      "182/182 - 0s - loss: 0.1958 - val_loss: 0.2252\n",
      "Epoch 12/2000\n",
      "182/182 - 0s - loss: 0.1827 - val_loss: 0.2216\n",
      "Epoch 13/2000\n",
      "182/182 - 0s - loss: 0.1852 - val_loss: 0.2181\n",
      "Epoch 14/2000\n",
      "182/182 - 0s - loss: 0.1749 - val_loss: 0.2156\n",
      "Epoch 15/2000\n",
      "182/182 - 0s - loss: 0.1652 - val_loss: 0.2124\n",
      "Epoch 16/2000\n",
      "182/182 - 0s - loss: 0.1589 - val_loss: 0.2095\n",
      "Epoch 17/2000\n",
      "182/182 - 0s - loss: 0.1472 - val_loss: 0.2067\n",
      "Epoch 18/2000\n",
      "182/182 - 0s - loss: 0.1430 - val_loss: 0.2034\n",
      "Epoch 19/2000\n",
      "182/182 - 0s - loss: 0.1325 - val_loss: 0.2000\n",
      "Epoch 20/2000\n",
      "182/182 - 0s - loss: 0.1308 - val_loss: 0.1972\n",
      "Epoch 21/2000\n",
      "182/182 - 0s - loss: 0.1169 - val_loss: 0.1946\n",
      "Epoch 22/2000\n",
      "182/182 - 0s - loss: 0.1155 - val_loss: 0.1924\n",
      "Epoch 23/2000\n",
      "182/182 - 0s - loss: 0.1035 - val_loss: 0.1896\n",
      "Epoch 24/2000\n",
      "182/182 - 0s - loss: 0.1022 - val_loss: 0.1865\n",
      "Epoch 25/2000\n",
      "182/182 - 0s - loss: 0.0960 - val_loss: 0.1843\n",
      "Epoch 26/2000\n",
      "182/182 - 0s - loss: 0.0881 - val_loss: 0.1824\n",
      "Epoch 27/2000\n",
      "182/182 - 0s - loss: 0.0867 - val_loss: 0.1814\n",
      "Epoch 28/2000\n",
      "182/182 - 0s - loss: 0.0802 - val_loss: 0.1791\n",
      "Epoch 29/2000\n",
      "182/182 - 0s - loss: 0.0750 - val_loss: 0.1774\n",
      "Epoch 30/2000\n",
      "182/182 - 0s - loss: 0.0679 - val_loss: 0.1755\n",
      "Epoch 31/2000\n",
      "182/182 - 0s - loss: 0.0648 - val_loss: 0.1746\n",
      "Epoch 32/2000\n",
      "182/182 - 0s - loss: 0.0652 - val_loss: 0.1724\n",
      "Epoch 33/2000\n",
      "182/182 - 0s - loss: 0.0561 - val_loss: 0.1712\n",
      "Epoch 34/2000\n",
      "182/182 - 0s - loss: 0.0546 - val_loss: 0.1701\n",
      "Epoch 35/2000\n",
      "182/182 - 0s - loss: 0.0545 - val_loss: 0.1691\n",
      "Epoch 36/2000\n",
      "182/182 - 0s - loss: 0.0491 - val_loss: 0.1684\n",
      "Epoch 37/2000\n",
      "182/182 - 0s - loss: 0.0429 - val_loss: 0.1669\n",
      "Epoch 38/2000\n",
      "182/182 - 0s - loss: 0.0408 - val_loss: 0.1656\n",
      "Epoch 39/2000\n",
      "182/182 - 0s - loss: 0.0437 - val_loss: 0.1642\n",
      "Epoch 40/2000\n",
      "182/182 - 0s - loss: 0.0378 - val_loss: 0.1642\n",
      "Epoch 41/2000\n",
      "182/182 - 0s - loss: 0.0358 - val_loss: 0.1619\n",
      "Epoch 42/2000\n",
      "182/182 - 0s - loss: 0.0322 - val_loss: 0.1609\n",
      "Epoch 43/2000\n",
      "182/182 - 0s - loss: 0.0277 - val_loss: 0.1596\n",
      "Epoch 44/2000\n",
      "182/182 - 0s - loss: 0.0255 - val_loss: 0.1592\n",
      "Epoch 45/2000\n",
      "182/182 - 0s - loss: 0.0254 - val_loss: 0.1586\n",
      "Epoch 46/2000\n",
      "182/182 - 0s - loss: 0.0271 - val_loss: 0.1573\n",
      "Epoch 47/2000\n",
      "182/182 - 0s - loss: 0.0248 - val_loss: 0.1569\n",
      "Epoch 48/2000\n",
      "182/182 - 0s - loss: 0.0211 - val_loss: 0.1571\n",
      "Epoch 49/2000\n",
      "182/182 - 0s - loss: 0.0206 - val_loss: 0.1554\n",
      "Epoch 50/2000\n",
      "182/182 - 0s - loss: 0.0175 - val_loss: 0.1546\n",
      "Epoch 51/2000\n",
      "182/182 - 0s - loss: 0.0181 - val_loss: 0.1537\n",
      "Epoch 52/2000\n",
      "182/182 - 0s - loss: 0.0167 - val_loss: 0.1545\n",
      "Epoch 53/2000\n",
      "182/182 - 0s - loss: 0.0173 - val_loss: 0.1552\n",
      "Epoch 54/2000\n",
      "182/182 - 0s - loss: 0.0157 - val_loss: 0.1533\n",
      "Epoch 55/2000\n",
      "182/182 - 0s - loss: 0.0142 - val_loss: 0.1525\n",
      "Epoch 56/2000\n",
      "182/182 - 0s - loss: 0.0146 - val_loss: 0.1521\n",
      "Epoch 57/2000\n",
      "182/182 - 0s - loss: 0.0134 - val_loss: 0.1521\n",
      "Epoch 58/2000\n",
      "182/182 - 0s - loss: 0.0131 - val_loss: 0.1526\n",
      "Epoch 59/2000\n",
      "182/182 - 0s - loss: 0.0132 - val_loss: 0.1511\n",
      "Epoch 60/2000\n",
      "182/182 - 0s - loss: 0.0108 - val_loss: 0.1498\n",
      "Epoch 61/2000\n",
      "182/182 - 0s - loss: 0.0121 - val_loss: 0.1495\n",
      "Epoch 62/2000\n",
      "182/182 - 0s - loss: 0.0089 - val_loss: 0.1495\n",
      "Epoch 63/2000\n",
      "182/182 - 0s - loss: 0.0116 - val_loss: 0.1499\n",
      "Epoch 64/2000\n",
      "182/182 - 0s - loss: 0.0092 - val_loss: 0.1504\n",
      "Epoch 65/2000\n",
      "182/182 - 0s - loss: 0.0090 - val_loss: 0.1499\n",
      "Epoch 66/2000\n",
      "182/182 - 0s - loss: 0.0087 - val_loss: 0.1489\n",
      "Epoch 67/2000\n",
      "182/182 - 0s - loss: 0.0100 - val_loss: 0.1487\n",
      "Epoch 68/2000\n",
      "182/182 - 0s - loss: 0.0093 - val_loss: 0.1488\n",
      "Epoch 69/2000\n",
      "182/182 - 0s - loss: 0.0069 - val_loss: 0.1492\n",
      "Epoch 70/2000\n",
      "182/182 - 0s - loss: 0.0091 - val_loss: 0.1487\n",
      "Epoch 71/2000\n",
      "182/182 - 0s - loss: 0.0075 - val_loss: 0.1483\n",
      "Epoch 72/2000\n",
      "182/182 - 0s - loss: 0.0070 - val_loss: 0.1478\n",
      "Epoch 73/2000\n",
      "182/182 - 0s - loss: 0.0071 - val_loss: 0.1478\n",
      "Epoch 74/2000\n",
      "182/182 - 0s - loss: 0.0076 - val_loss: 0.1479\n",
      "Epoch 75/2000\n",
      "182/182 - 0s - loss: 0.0060 - val_loss: 0.1479\n",
      "Epoch 76/2000\n",
      "182/182 - 0s - loss: 0.0064 - val_loss: 0.1477\n",
      "Epoch 77/2000\n",
      "182/182 - 0s - loss: 0.0062 - val_loss: 0.1475\n",
      "Epoch 78/2000\n",
      "182/182 - 0s - loss: 0.0059 - val_loss: 0.1469\n",
      "Epoch 79/2000\n",
      "182/182 - 0s - loss: 0.0076 - val_loss: 0.1470\n",
      "Epoch 80/2000\n",
      "182/182 - 0s - loss: 0.0051 - val_loss: 0.1473\n",
      "Epoch 81/2000\n",
      "182/182 - 0s - loss: 0.0066 - val_loss: 0.1475\n",
      "Epoch 82/2000\n",
      "182/182 - 0s - loss: 0.0052 - val_loss: 0.1472\n",
      "Epoch 83/2000\n",
      "182/182 - 0s - loss: 0.0049 - val_loss: 0.1470\n",
      "Epoch 84/2000\n",
      "182/182 - 0s - loss: 0.0045 - val_loss: 0.1470\n",
      "Epoch 85/2000\n",
      "182/182 - 0s - loss: 0.0043 - val_loss: 0.1472\n",
      "Epoch 86/2000\n",
      "182/182 - 0s - loss: 0.0052 - val_loss: 0.1478\n",
      "Epoch 87/2000\n",
      "182/182 - 0s - loss: 0.0058 - val_loss: 0.1472\n",
      "Epoch 88/2000\n",
      "182/182 - 0s - loss: 0.0041 - val_loss: 0.1464\n",
      "Epoch 89/2000\n",
      "182/182 - 0s - loss: 0.0039 - val_loss: 0.1462\n",
      "Epoch 90/2000\n",
      "182/182 - 0s - loss: 0.0052 - val_loss: 0.1465\n",
      "Epoch 91/2000\n",
      "182/182 - 0s - loss: 0.0049 - val_loss: 0.1467\n",
      "Epoch 92/2000\n",
      "182/182 - 0s - loss: 0.0031 - val_loss: 0.1462\n",
      "Epoch 93/2000\n",
      "182/182 - 0s - loss: 0.0037 - val_loss: 0.1457\n",
      "Epoch 94/2000\n",
      "182/182 - 0s - loss: 0.0039 - val_loss: 0.1457\n",
      "Epoch 95/2000\n",
      "182/182 - 0s - loss: 0.0040 - val_loss: 0.1458\n",
      "Epoch 96/2000\n",
      "182/182 - 0s - loss: 0.0033 - val_loss: 0.1461\n",
      "Epoch 97/2000\n",
      "182/182 - 0s - loss: 0.0044 - val_loss: 0.1462\n",
      "Epoch 98/2000\n",
      "182/182 - 0s - loss: 0.0035 - val_loss: 0.1462\n",
      "Epoch 99/2000\n",
      "182/182 - 0s - loss: 0.0025 - val_loss: 0.1466\n",
      "Epoch 100/2000\n",
      "182/182 - 0s - loss: 0.0027 - val_loss: 0.1466\n",
      "Epoch 101/2000\n",
      "182/182 - 0s - loss: 0.0032 - val_loss: 0.1467\n",
      "Epoch 102/2000\n",
      "182/182 - 0s - loss: 0.0021 - val_loss: 0.1474\n",
      "Epoch 103/2000\n",
      "182/182 - 0s - loss: 0.0038 - val_loss: 0.1470\n",
      "Epoch 104/2000\n",
      "182/182 - 0s - loss: 0.0044 - val_loss: 0.1474\n",
      "Epoch 105/2000\n",
      "182/182 - 0s - loss: 0.0026 - val_loss: 0.1478\n",
      "Epoch 106/2000\n",
      "182/182 - 0s - loss: 0.0026 - val_loss: 0.1470\n",
      "Epoch 107/2000\n",
      "182/182 - 0s - loss: 0.0027 - val_loss: 0.1464\n",
      "Epoch 108/2000\n",
      "182/182 - 0s - loss: 0.0028 - val_loss: 0.1463\n",
      "Epoch 109/2000\n",
      "182/182 - 0s - loss: 0.0017 - val_loss: 0.1463\n",
      "Epoch 110/2000\n",
      "182/182 - 0s - loss: 0.0023 - val_loss: 0.1462\n",
      "Epoch 111/2000\n",
      "182/182 - 0s - loss: 0.0021 - val_loss: 0.1462\n",
      "Epoch 112/2000\n",
      "182/182 - 0s - loss: 0.0030 - val_loss: 0.1463\n",
      "Epoch 113/2000\n",
      "182/182 - 0s - loss: 0.0020 - val_loss: 0.1462\n",
      "Epoch 114/2000\n",
      "182/182 - 0s - loss: 0.0025 - val_loss: 0.1460\n",
      "Epoch 115/2000\n",
      "182/182 - 0s - loss: 0.0026 - val_loss: 0.1459\n",
      "Epoch 116/2000\n",
      "182/182 - 0s - loss: 0.0015 - val_loss: 0.1459\n",
      "Epoch 117/2000\n",
      "182/182 - 0s - loss: 0.0022 - val_loss: 0.1460\n",
      "Epoch 118/2000\n",
      "182/182 - 0s - loss: 0.0022 - val_loss: 0.1462\n",
      "Epoch 119/2000\n",
      "182/182 - 0s - loss: 0.0020 - val_loss: 0.1461\n",
      "Epoch 120/2000\n",
      "182/182 - 0s - loss: 0.0018 - val_loss: 0.1461\n",
      "Epoch 121/2000\n",
      "182/182 - 0s - loss: 0.0017 - val_loss: 0.1462\n",
      "Epoch 122/2000\n",
      "182/182 - 0s - loss: 0.0026 - val_loss: 0.1463\n",
      "Epoch 123/2000\n",
      "182/182 - 0s - loss: 0.0021 - val_loss: 0.1462\n",
      "Epoch 124/2000\n",
      "182/182 - 0s - loss: 0.0018 - val_loss: 0.1462\n",
      "Epoch 125/2000\n",
      "182/182 - 0s - loss: 0.0018 - val_loss: 0.1460\n",
      "Epoch 126/2000\n",
      "182/182 - 0s - loss: 0.0017 - val_loss: 0.1461\n",
      "Epoch 127/2000\n",
      "182/182 - 0s - loss: 0.0017 - val_loss: 0.1460\n",
      "Epoch 128/2000\n",
      "182/182 - 0s - loss: 0.0019 - val_loss: 0.1460\n",
      "Epoch 129/2000\n",
      "182/182 - 0s - loss: 0.0015 - val_loss: 0.1460\n",
      "Epoch 130/2000\n",
      "182/182 - 0s - loss: 0.0019 - val_loss: 0.1459\n",
      "Epoch 131/2000\n",
      "182/182 - 0s - loss: 0.0013 - val_loss: 0.1460\n",
      "Epoch 132/2000\n",
      "182/182 - 0s - loss: 0.0014 - val_loss: 0.1461\n",
      "Epoch 133/2000\n",
      "182/182 - 0s - loss: 0.0013 - val_loss: 0.1462\n",
      "Evaluation:\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.1462103089873858\n",
      "AUC: 0.8620037807183365\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Beginning cross validation for candidate number 2\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Total size of the training set: 182\n",
      "Total size of the validation set: 46\n",
      "Class frequencies in the training set: [0.5 0.5]\n",
      "Class frequencies in the validation set: [0.5 0.5]\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Beginning the tree building procedure\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Fitting candidate number 3 in shuffle 1 with parameters\n",
      "\n",
      "{'activation': 'elu', 'batch_size': 64, 'callbacks': [<tensorflow.python.keras.callbacks.EarlyStopping object at 0x0000019C07A8A6C8>], 'dropout': 0.3, 'epochs': 2000, 'filters': 10, 'input_shape': None, 'kernel_size': (3, 9), 'learning_rate': 0.001, 'loss': 'mse', 'n_classes': 2, 'n_layers': 1, 'pool_size': (2, 2), 'verbose': 2}\n",
      "Train on 182 samples, validate on 46 samples\n",
      "Epoch 1/2000\n",
      "182/182 - 1s - loss: 0.2349 - val_loss: 0.2449\n",
      "Epoch 2/2000\n",
      "182/182 - 0s - loss: 0.2289 - val_loss: 0.2385\n",
      "Epoch 3/2000\n",
      "182/182 - 0s - loss: 0.2031 - val_loss: 0.2338\n",
      "Epoch 4/2000\n",
      "182/182 - 0s - loss: 0.1799 - val_loss: 0.2281\n",
      "Epoch 5/2000\n",
      "182/182 - 0s - loss: 0.1637 - val_loss: 0.2209\n",
      "Epoch 6/2000\n",
      "182/182 - 0s - loss: 0.1467 - val_loss: 0.2133\n",
      "Epoch 7/2000\n",
      "182/182 - 0s - loss: 0.1424 - val_loss: 0.2064\n",
      "Epoch 8/2000\n",
      "182/182 - 0s - loss: 0.1240 - val_loss: 0.2027\n",
      "Epoch 9/2000\n",
      "182/182 - 0s - loss: 0.1166 - val_loss: 0.1986\n",
      "Epoch 10/2000\n",
      "182/182 - 0s - loss: 0.0983 - val_loss: 0.1948\n",
      "Epoch 11/2000\n",
      "182/182 - 0s - loss: 0.0943 - val_loss: 0.1908\n",
      "Epoch 12/2000\n",
      "182/182 - 0s - loss: 0.0743 - val_loss: 0.1847\n",
      "Epoch 13/2000\n",
      "182/182 - 0s - loss: 0.0718 - val_loss: 0.1811\n",
      "Epoch 14/2000\n",
      "182/182 - 0s - loss: 0.0633 - val_loss: 0.1784\n",
      "Epoch 15/2000\n",
      "182/182 - 0s - loss: 0.0587 - val_loss: 0.1741\n",
      "Epoch 16/2000\n",
      "182/182 - 0s - loss: 0.0546 - val_loss: 0.1716\n",
      "Epoch 17/2000\n",
      "182/182 - 0s - loss: 0.0450 - val_loss: 0.1698\n",
      "Epoch 18/2000\n",
      "182/182 - 0s - loss: 0.0414 - val_loss: 0.1671\n",
      "Epoch 19/2000\n",
      "182/182 - 0s - loss: 0.0387 - val_loss: 0.1656\n",
      "Epoch 20/2000\n",
      "182/182 - 0s - loss: 0.0320 - val_loss: 0.1629\n",
      "Epoch 21/2000\n",
      "182/182 - 0s - loss: 0.0285 - val_loss: 0.1610\n",
      "Epoch 22/2000\n",
      "182/182 - 0s - loss: 0.0224 - val_loss: 0.1589\n",
      "Epoch 23/2000\n",
      "182/182 - 0s - loss: 0.0225 - val_loss: 0.1580\n",
      "Epoch 24/2000\n",
      "182/182 - 0s - loss: 0.0189 - val_loss: 0.1576\n",
      "Epoch 25/2000\n",
      "182/182 - 0s - loss: 0.0198 - val_loss: 0.1552\n",
      "Epoch 26/2000\n",
      "182/182 - 0s - loss: 0.0178 - val_loss: 0.1533\n",
      "Epoch 27/2000\n",
      "182/182 - 0s - loss: 0.0160 - val_loss: 0.1526\n",
      "Epoch 28/2000\n",
      "182/182 - 0s - loss: 0.0172 - val_loss: 0.1517\n",
      "Epoch 29/2000\n",
      "182/182 - 0s - loss: 0.0173 - val_loss: 0.1503\n",
      "Epoch 30/2000\n",
      "182/182 - 0s - loss: 0.0146 - val_loss: 0.1486\n",
      "Epoch 31/2000\n",
      "182/182 - 0s - loss: 0.0117 - val_loss: 0.1480\n",
      "Epoch 32/2000\n",
      "182/182 - 0s - loss: 0.0113 - val_loss: 0.1483\n",
      "Epoch 33/2000\n",
      "182/182 - 0s - loss: 0.0115 - val_loss: 0.1516\n",
      "Epoch 34/2000\n",
      "182/182 - 0s - loss: 0.0091 - val_loss: 0.1510\n",
      "Epoch 35/2000\n",
      "182/182 - 0s - loss: 0.0096 - val_loss: 0.1476\n",
      "Epoch 36/2000\n",
      "182/182 - 0s - loss: 0.0100 - val_loss: 0.1468\n",
      "Epoch 37/2000\n",
      "182/182 - 0s - loss: 0.0079 - val_loss: 0.1462\n",
      "Epoch 38/2000\n",
      "182/182 - 0s - loss: 0.0071 - val_loss: 0.1475\n",
      "Epoch 39/2000\n",
      "182/182 - 0s - loss: 0.0069 - val_loss: 0.1477\n",
      "Epoch 40/2000\n",
      "182/182 - 0s - loss: 0.0053 - val_loss: 0.1473\n",
      "Epoch 41/2000\n",
      "182/182 - 0s - loss: 0.0054 - val_loss: 0.1479\n",
      "Epoch 42/2000\n",
      "182/182 - 0s - loss: 0.0050 - val_loss: 0.1491\n",
      "Epoch 43/2000\n",
      "182/182 - 0s - loss: 0.0048 - val_loss: 0.1496\n",
      "Epoch 44/2000\n",
      "182/182 - 0s - loss: 0.0039 - val_loss: 0.1495\n",
      "Epoch 45/2000\n",
      "182/182 - 0s - loss: 0.0057 - val_loss: 0.1508\n",
      "Epoch 46/2000\n",
      "182/182 - 0s - loss: 0.0054 - val_loss: 0.1496\n",
      "Epoch 47/2000\n",
      "182/182 - 0s - loss: 0.0048 - val_loss: 0.1481\n",
      "Epoch 48/2000\n",
      "182/182 - 0s - loss: 0.0042 - val_loss: 0.1479\n",
      "Epoch 49/2000\n",
      "182/182 - 0s - loss: 0.0052 - val_loss: 0.1486\n",
      "Epoch 50/2000\n",
      "182/182 - 0s - loss: 0.0049 - val_loss: 0.1523\n",
      "Epoch 51/2000\n",
      "182/182 - 0s - loss: 0.0040 - val_loss: 0.1531\n",
      "Epoch 52/2000\n",
      "182/182 - 0s - loss: 0.0041 - val_loss: 0.1497\n",
      "Epoch 53/2000\n",
      "182/182 - 0s - loss: 0.0043 - val_loss: 0.1480\n",
      "Epoch 54/2000\n",
      "182/182 - 0s - loss: 0.0039 - val_loss: 0.1481\n",
      "Epoch 55/2000\n",
      "182/182 - 0s - loss: 0.0036 - val_loss: 0.1484\n",
      "Epoch 56/2000\n",
      "182/182 - 0s - loss: 0.0034 - val_loss: 0.1494\n",
      "Epoch 57/2000\n",
      "182/182 - 0s - loss: 0.0039 - val_loss: 0.1515\n",
      "Epoch 58/2000\n",
      "182/182 - 0s - loss: 0.0025 - val_loss: 0.1524\n",
      "Epoch 59/2000\n",
      "182/182 - 0s - loss: 0.0033 - val_loss: 0.1484\n",
      "Epoch 60/2000\n",
      "182/182 - 0s - loss: 0.0027 - val_loss: 0.1470\n",
      "Epoch 61/2000\n",
      "182/182 - 0s - loss: 0.0029 - val_loss: 0.1467\n",
      "Epoch 62/2000\n",
      "182/182 - 0s - loss: 0.0029 - val_loss: 0.1477\n",
      "Epoch 63/2000\n",
      "182/182 - 0s - loss: 0.0024 - val_loss: 0.1495\n",
      "Epoch 64/2000\n",
      "182/182 - 0s - loss: 0.0026 - val_loss: 0.1498\n",
      "Epoch 65/2000\n",
      "182/182 - 0s - loss: 0.0015 - val_loss: 0.1499\n",
      "Epoch 66/2000\n",
      "182/182 - 0s - loss: 0.0017 - val_loss: 0.1501\n",
      "Epoch 67/2000\n",
      "182/182 - 0s - loss: 0.0018 - val_loss: 0.1503\n",
      "Epoch 68/2000\n",
      "182/182 - 0s - loss: 0.0017 - val_loss: 0.1507\n",
      "Epoch 69/2000\n",
      "182/182 - 0s - loss: 0.0025 - val_loss: 0.1514\n",
      "Epoch 70/2000\n",
      "182/182 - 0s - loss: 0.0017 - val_loss: 0.1522\n",
      "Epoch 71/2000\n",
      "182/182 - 0s - loss: 0.0013 - val_loss: 0.1528\n",
      "Epoch 72/2000\n",
      "182/182 - 0s - loss: 0.0023 - val_loss: 0.1523\n",
      "Epoch 73/2000\n",
      "182/182 - 0s - loss: 0.0017 - val_loss: 0.1518\n",
      "Epoch 74/2000\n",
      "182/182 - 0s - loss: 0.0021 - val_loss: 0.1523\n",
      "Epoch 75/2000\n",
      "182/182 - 0s - loss: 0.0023 - val_loss: 0.1542\n",
      "Epoch 76/2000\n",
      "182/182 - 0s - loss: 0.0016 - val_loss: 0.1565\n",
      "Epoch 77/2000\n",
      "182/182 - 0s - loss: 0.0013 - val_loss: 0.1567\n",
      "Evaluation:\n",
      "\n",
      "MSE: 0.15666437770016506\n",
      "AUC: 0.8695652173913044\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Beginning cross validation for candidate number 3\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Total size of the training set: 183\n",
      "Total size of the validation set: 45\n",
      "Class frequencies in the training set: [0.49726776 0.50273224]\n",
      "Class frequencies in the validation set: [0.51111111 0.48888889]\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Beginning the tree building procedure\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Fitting candidate number 4 in shuffle 1 with parameters\n",
      "\n",
      "{'activation': 'elu', 'batch_size': 64, 'callbacks': [<tensorflow.python.keras.callbacks.EarlyStopping object at 0x0000019C07A8A6C8>], 'dropout': 0.3, 'epochs': 2000, 'filters': 10, 'input_shape': None, 'kernel_size': (3, 9), 'learning_rate': 0.001, 'loss': 'mse', 'n_classes': 2, 'n_layers': 1, 'pool_size': (2, 2), 'verbose': 2}\n",
      "Train on 183 samples, validate on 45 samples\n",
      "Epoch 1/2000\n",
      "183/183 - 1s - loss: 0.2501 - val_loss: 0.2493\n",
      "Epoch 2/2000\n",
      "183/183 - 0s - loss: 0.2479 - val_loss: 0.2470\n",
      "Epoch 3/2000\n",
      "183/183 - 0s - loss: 0.2417 - val_loss: 0.2450\n",
      "Epoch 4/2000\n",
      "183/183 - 0s - loss: 0.2363 - val_loss: 0.2427\n",
      "Epoch 5/2000\n",
      "183/183 - 0s - loss: 0.2314 - val_loss: 0.2396\n",
      "Epoch 6/2000\n",
      "183/183 - 0s - loss: 0.2240 - val_loss: 0.2368\n",
      "Epoch 7/2000\n",
      "183/183 - 0s - loss: 0.2196 - val_loss: 0.2336\n",
      "Epoch 8/2000\n",
      "183/183 - 0s - loss: 0.2161 - val_loss: 0.2303\n",
      "Epoch 9/2000\n",
      "183/183 - 0s - loss: 0.2088 - val_loss: 0.2273\n",
      "Epoch 10/2000\n",
      "183/183 - 0s - loss: 0.2046 - val_loss: 0.2249\n",
      "Epoch 11/2000\n",
      "183/183 - 0s - loss: 0.1954 - val_loss: 0.2222\n",
      "Epoch 12/2000\n",
      "183/183 - 0s - loss: 0.1901 - val_loss: 0.2195\n",
      "Epoch 13/2000\n",
      "183/183 - 0s - loss: 0.1816 - val_loss: 0.2167\n",
      "Epoch 14/2000\n",
      "183/183 - 0s - loss: 0.1695 - val_loss: 0.2130\n",
      "Epoch 15/2000\n",
      "183/183 - 0s - loss: 0.1635 - val_loss: 0.2093\n",
      "Epoch 16/2000\n",
      "183/183 - 0s - loss: 0.1551 - val_loss: 0.2078\n",
      "Epoch 17/2000\n",
      "183/183 - 0s - loss: 0.1517 - val_loss: 0.2044\n",
      "Epoch 18/2000\n",
      "183/183 - 0s - loss: 0.1415 - val_loss: 0.2013\n",
      "Epoch 19/2000\n",
      "183/183 - 0s - loss: 0.1322 - val_loss: 0.1994\n",
      "Epoch 20/2000\n",
      "183/183 - 0s - loss: 0.1310 - val_loss: 0.1974\n",
      "Epoch 21/2000\n",
      "183/183 - 0s - loss: 0.1216 - val_loss: 0.1955\n",
      "Epoch 22/2000\n",
      "183/183 - 0s - loss: 0.1179 - val_loss: 0.1931\n",
      "Epoch 23/2000\n",
      "183/183 - 0s - loss: 0.1051 - val_loss: 0.1902\n",
      "Epoch 24/2000\n",
      "183/183 - 0s - loss: 0.1052 - val_loss: 0.1884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/2000\n",
      "183/183 - 0s - loss: 0.0939 - val_loss: 0.1874\n",
      "Epoch 26/2000\n",
      "183/183 - 0s - loss: 0.0907 - val_loss: 0.1868\n",
      "Epoch 27/2000\n",
      "183/183 - 0s - loss: 0.0823 - val_loss: 0.1841\n",
      "Epoch 28/2000\n",
      "183/183 - 0s - loss: 0.0756 - val_loss: 0.1822\n",
      "Epoch 29/2000\n",
      "183/183 - 0s - loss: 0.0717 - val_loss: 0.1814\n",
      "Epoch 30/2000\n",
      "183/183 - 0s - loss: 0.0705 - val_loss: 0.1829\n",
      "Epoch 31/2000\n",
      "183/183 - 0s - loss: 0.0676 - val_loss: 0.1811\n",
      "Epoch 32/2000\n",
      "183/183 - 0s - loss: 0.0643 - val_loss: 0.1782\n",
      "Epoch 33/2000\n",
      "183/183 - 0s - loss: 0.0585 - val_loss: 0.1787\n",
      "Epoch 34/2000\n",
      "183/183 - 0s - loss: 0.0514 - val_loss: 0.1779\n",
      "Epoch 35/2000\n",
      "183/183 - 0s - loss: 0.0473 - val_loss: 0.1765\n",
      "Epoch 36/2000\n",
      "183/183 - 0s - loss: 0.0474 - val_loss: 0.1742\n",
      "Epoch 37/2000\n",
      "183/183 - 0s - loss: 0.0430 - val_loss: 0.1744\n",
      "Epoch 38/2000\n",
      "183/183 - 0s - loss: 0.0441 - val_loss: 0.1763\n",
      "Epoch 39/2000\n",
      "183/183 - 0s - loss: 0.0370 - val_loss: 0.1758\n",
      "Epoch 40/2000\n",
      "183/183 - 0s - loss: 0.0333 - val_loss: 0.1749\n",
      "Epoch 41/2000\n",
      "183/183 - 0s - loss: 0.0324 - val_loss: 0.1729\n",
      "Epoch 42/2000\n",
      "183/183 - 0s - loss: 0.0307 - val_loss: 0.1719\n",
      "Epoch 43/2000\n",
      "183/183 - 0s - loss: 0.0284 - val_loss: 0.1728\n",
      "Epoch 44/2000\n",
      "183/183 - 0s - loss: 0.0265 - val_loss: 0.1736\n",
      "Epoch 45/2000\n",
      "183/183 - 0s - loss: 0.0288 - val_loss: 0.1757\n",
      "Epoch 46/2000\n",
      "183/183 - 0s - loss: 0.0242 - val_loss: 0.1731\n",
      "Epoch 47/2000\n",
      "183/183 - 0s - loss: 0.0240 - val_loss: 0.1700\n",
      "Epoch 48/2000\n",
      "183/183 - 0s - loss: 0.0231 - val_loss: 0.1718\n",
      "Epoch 49/2000\n",
      "183/183 - 0s - loss: 0.0185 - val_loss: 0.1740\n",
      "Epoch 50/2000\n",
      "183/183 - 0s - loss: 0.0205 - val_loss: 0.1727\n",
      "Epoch 51/2000\n",
      "183/183 - 0s - loss: 0.0187 - val_loss: 0.1702\n",
      "Epoch 52/2000\n",
      "183/183 - 0s - loss: 0.0180 - val_loss: 0.1684\n",
      "Epoch 53/2000\n",
      "183/183 - 0s - loss: 0.0149 - val_loss: 0.1705\n",
      "Epoch 54/2000\n",
      "183/183 - 0s - loss: 0.0151 - val_loss: 0.1703\n",
      "Epoch 55/2000\n",
      "183/183 - 0s - loss: 0.0154 - val_loss: 0.1698\n",
      "Epoch 56/2000\n",
      "183/183 - 0s - loss: 0.0150 - val_loss: 0.1710\n",
      "Epoch 57/2000\n",
      "183/183 - 0s - loss: 0.0134 - val_loss: 0.1714\n",
      "Epoch 58/2000\n",
      "183/183 - 0s - loss: 0.0125 - val_loss: 0.1714\n",
      "Epoch 59/2000\n",
      "183/183 - 0s - loss: 0.0122 - val_loss: 0.1722\n",
      "Epoch 60/2000\n",
      "183/183 - 0s - loss: 0.0121 - val_loss: 0.1721\n",
      "Epoch 61/2000\n",
      "183/183 - 0s - loss: 0.0106 - val_loss: 0.1723\n",
      "Epoch 62/2000\n",
      "183/183 - 0s - loss: 0.0105 - val_loss: 0.1729\n",
      "Epoch 63/2000\n",
      "183/183 - 0s - loss: 0.0124 - val_loss: 0.1758\n",
      "Epoch 64/2000\n",
      "183/183 - 0s - loss: 0.0096 - val_loss: 0.1790\n",
      "Epoch 65/2000\n",
      "183/183 - 0s - loss: 0.0091 - val_loss: 0.1759\n",
      "Epoch 66/2000\n",
      "183/183 - 0s - loss: 0.0079 - val_loss: 0.1724\n",
      "Epoch 67/2000\n",
      "183/183 - 0s - loss: 0.0090 - val_loss: 0.1730\n",
      "Epoch 68/2000\n",
      "183/183 - 0s - loss: 0.0082 - val_loss: 0.1773\n",
      "Epoch 69/2000\n",
      "183/183 - 0s - loss: 0.0087 - val_loss: 0.1781\n",
      "Epoch 70/2000\n",
      "183/183 - 0s - loss: 0.0074 - val_loss: 0.1778\n",
      "Epoch 71/2000\n",
      "183/183 - 0s - loss: 0.0064 - val_loss: 0.1763\n",
      "Epoch 72/2000\n",
      "183/183 - 0s - loss: 0.0075 - val_loss: 0.1741\n",
      "Epoch 73/2000\n",
      "183/183 - 0s - loss: 0.0073 - val_loss: 0.1754\n",
      "Epoch 74/2000\n",
      "183/183 - 0s - loss: 0.0064 - val_loss: 0.1784\n",
      "Epoch 75/2000\n",
      "183/183 - 0s - loss: 0.0058 - val_loss: 0.1812\n",
      "Epoch 76/2000\n",
      "183/183 - 0s - loss: 0.0058 - val_loss: 0.1795\n",
      "Epoch 77/2000\n",
      "183/183 - 0s - loss: 0.0052 - val_loss: 0.1775\n",
      "Epoch 78/2000\n",
      "183/183 - 0s - loss: 0.0050 - val_loss: 0.1786\n",
      "Epoch 79/2000\n",
      "183/183 - 0s - loss: 0.0062 - val_loss: 0.1836\n",
      "Epoch 80/2000\n",
      "183/183 - 0s - loss: 0.0043 - val_loss: 0.1872\n",
      "Epoch 81/2000\n",
      "183/183 - 0s - loss: 0.0045 - val_loss: 0.1853\n",
      "Epoch 82/2000\n",
      "183/183 - 0s - loss: 0.0053 - val_loss: 0.1815\n",
      "Epoch 83/2000\n",
      "183/183 - 0s - loss: 0.0048 - val_loss: 0.1793\n",
      "Epoch 84/2000\n",
      "183/183 - 0s - loss: 0.0048 - val_loss: 0.1792\n",
      "Epoch 85/2000\n",
      "183/183 - 0s - loss: 0.0040 - val_loss: 0.1817\n",
      "Epoch 86/2000\n",
      "183/183 - 0s - loss: 0.0044 - val_loss: 0.1856\n",
      "Epoch 87/2000\n",
      "183/183 - 0s - loss: 0.0050 - val_loss: 0.1872\n",
      "Epoch 88/2000\n",
      "183/183 - 0s - loss: 0.0039 - val_loss: 0.1857\n",
      "Epoch 89/2000\n",
      "183/183 - 0s - loss: 0.0043 - val_loss: 0.1823\n",
      "Epoch 90/2000\n",
      "183/183 - 0s - loss: 0.0037 - val_loss: 0.1818\n",
      "Epoch 91/2000\n",
      "183/183 - 0s - loss: 0.0036 - val_loss: 0.1850\n",
      "Epoch 92/2000\n",
      "183/183 - 0s - loss: 0.0034 - val_loss: 0.1869\n",
      "Evaluation:\n",
      "\n",
      "MSE: 0.18693661457385882\n",
      "AUC: 0.8102766798418972\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Beginning cross validation for candidate number 4\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Total size of the training set: 183\n",
      "Total size of the validation set: 45\n",
      "Class frequencies in the training set: [0.50273224 0.49726776]\n",
      "Class frequencies in the validation set: [0.48888889 0.51111111]\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Beginning the tree building procedure\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Fitting candidate number 5 in shuffle 1 with parameters\n",
      "\n",
      "{'activation': 'elu', 'batch_size': 64, 'callbacks': [<tensorflow.python.keras.callbacks.EarlyStopping object at 0x0000019C07A8A6C8>], 'dropout': 0.3, 'epochs': 2000, 'filters': 10, 'input_shape': None, 'kernel_size': (3, 9), 'learning_rate': 0.001, 'loss': 'mse', 'n_classes': 2, 'n_layers': 1, 'pool_size': (2, 2), 'verbose': 2}\n",
      "Train on 183 samples, validate on 45 samples\n",
      "Epoch 1/2000\n",
      "183/183 - 1s - loss: 0.2564 - val_loss: 0.2415\n",
      "Epoch 2/2000\n",
      "183/183 - 0s - loss: 0.2379 - val_loss: 0.2392\n",
      "Epoch 3/2000\n",
      "183/183 - 0s - loss: 0.2318 - val_loss: 0.2376\n",
      "Epoch 4/2000\n",
      "183/183 - 0s - loss: 0.2206 - val_loss: 0.2317\n",
      "Epoch 5/2000\n",
      "183/183 - 0s - loss: 0.2145 - val_loss: 0.2270\n",
      "Epoch 6/2000\n",
      "183/183 - 0s - loss: 0.2017 - val_loss: 0.2231\n",
      "Epoch 7/2000\n",
      "183/183 - 0s - loss: 0.1923 - val_loss: 0.2188\n",
      "Epoch 8/2000\n",
      "183/183 - 0s - loss: 0.1873 - val_loss: 0.2150\n",
      "Epoch 9/2000\n",
      "183/183 - 0s - loss: 0.1731 - val_loss: 0.2105\n",
      "Epoch 10/2000\n",
      "183/183 - 0s - loss: 0.1619 - val_loss: 0.2049\n",
      "Epoch 11/2000\n",
      "183/183 - 0s - loss: 0.1561 - val_loss: 0.2005\n",
      "Epoch 12/2000\n",
      "183/183 - 0s - loss: 0.1468 - val_loss: 0.1962\n",
      "Epoch 13/2000\n",
      "183/183 - 0s - loss: 0.1335 - val_loss: 0.1940\n",
      "Epoch 14/2000\n",
      "183/183 - 0s - loss: 0.1230 - val_loss: 0.1893\n",
      "Epoch 15/2000\n",
      "183/183 - 0s - loss: 0.1164 - val_loss: 0.1861\n",
      "Epoch 16/2000\n",
      "183/183 - 0s - loss: 0.1063 - val_loss: 0.1835\n",
      "Epoch 17/2000\n",
      "183/183 - 0s - loss: 0.0946 - val_loss: 0.1807\n",
      "Epoch 18/2000\n",
      "183/183 - 0s - loss: 0.0888 - val_loss: 0.1782\n",
      "Epoch 19/2000\n",
      "183/183 - 0s - loss: 0.0854 - val_loss: 0.1765\n",
      "Epoch 20/2000\n",
      "183/183 - 0s - loss: 0.0779 - val_loss: 0.1771\n",
      "Epoch 21/2000\n",
      "183/183 - 0s - loss: 0.0681 - val_loss: 0.1746\n",
      "Epoch 22/2000\n",
      "183/183 - 0s - loss: 0.0641 - val_loss: 0.1718\n",
      "Epoch 23/2000\n",
      "183/183 - 0s - loss: 0.0565 - val_loss: 0.1699\n",
      "Epoch 24/2000\n",
      "183/183 - 0s - loss: 0.0532 - val_loss: 0.1702\n",
      "Epoch 25/2000\n",
      "183/183 - 0s - loss: 0.0546 - val_loss: 0.1707\n",
      "Epoch 26/2000\n",
      "183/183 - 0s - loss: 0.0454 - val_loss: 0.1691\n",
      "Epoch 27/2000\n",
      "183/183 - 0s - loss: 0.0465 - val_loss: 0.1694\n",
      "Epoch 28/2000\n",
      "183/183 - 0s - loss: 0.0384 - val_loss: 0.1716\n",
      "Epoch 29/2000\n",
      "183/183 - 0s - loss: 0.0328 - val_loss: 0.1715\n",
      "Epoch 30/2000\n",
      "183/183 - 0s - loss: 0.0385 - val_loss: 0.1691\n",
      "Epoch 31/2000\n",
      "183/183 - 0s - loss: 0.0310 - val_loss: 0.1681\n",
      "Epoch 32/2000\n",
      "183/183 - 0s - loss: 0.0304 - val_loss: 0.1696\n",
      "Epoch 33/2000\n",
      "183/183 - 0s - loss: 0.0256 - val_loss: 0.1696\n",
      "Epoch 34/2000\n",
      "183/183 - 0s - loss: 0.0245 - val_loss: 0.1669\n",
      "Epoch 35/2000\n",
      "183/183 - 0s - loss: 0.0235 - val_loss: 0.1657\n",
      "Epoch 36/2000\n",
      "183/183 - 0s - loss: 0.0196 - val_loss: 0.1686\n",
      "Epoch 37/2000\n",
      "183/183 - 0s - loss: 0.0217 - val_loss: 0.1676\n",
      "Epoch 38/2000\n",
      "183/183 - 0s - loss: 0.0190 - val_loss: 0.1635\n",
      "Epoch 39/2000\n",
      "183/183 - 0s - loss: 0.0222 - val_loss: 0.1632\n",
      "Epoch 40/2000\n",
      "183/183 - 0s - loss: 0.0146 - val_loss: 0.1649\n",
      "Epoch 41/2000\n",
      "183/183 - 0s - loss: 0.0141 - val_loss: 0.1690\n",
      "Epoch 42/2000\n",
      "183/183 - 0s - loss: 0.0163 - val_loss: 0.1695\n",
      "Epoch 43/2000\n",
      "183/183 - 0s - loss: 0.0153 - val_loss: 0.1656\n",
      "Epoch 44/2000\n",
      "183/183 - 0s - loss: 0.0117 - val_loss: 0.1646\n",
      "Epoch 45/2000\n",
      "183/183 - 0s - loss: 0.0098 - val_loss: 0.1653\n",
      "Epoch 46/2000\n",
      "183/183 - 0s - loss: 0.0100 - val_loss: 0.1664\n",
      "Epoch 47/2000\n",
      "183/183 - 0s - loss: 0.0112 - val_loss: 0.1678\n",
      "Epoch 48/2000\n",
      "183/183 - 0s - loss: 0.0117 - val_loss: 0.1689\n",
      "Epoch 49/2000\n",
      "183/183 - 0s - loss: 0.0109 - val_loss: 0.1659\n",
      "Epoch 50/2000\n",
      "183/183 - 0s - loss: 0.0100 - val_loss: 0.1649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/2000\n",
      "183/183 - 0s - loss: 0.0102 - val_loss: 0.1655\n",
      "Epoch 52/2000\n",
      "183/183 - 0s - loss: 0.0082 - val_loss: 0.1676\n",
      "Epoch 53/2000\n",
      "183/183 - 0s - loss: 0.0080 - val_loss: 0.1686\n",
      "Epoch 54/2000\n",
      "183/183 - 0s - loss: 0.0065 - val_loss: 0.1677\n",
      "Epoch 55/2000\n",
      "183/183 - 0s - loss: 0.0071 - val_loss: 0.1661\n",
      "Epoch 56/2000\n",
      "183/183 - 0s - loss: 0.0096 - val_loss: 0.1651\n",
      "Epoch 57/2000\n",
      "183/183 - 0s - loss: 0.0056 - val_loss: 0.1671\n",
      "Epoch 58/2000\n",
      "183/183 - 0s - loss: 0.0079 - val_loss: 0.1717\n",
      "Epoch 59/2000\n",
      "183/183 - 0s - loss: 0.0073 - val_loss: 0.1714\n",
      "Epoch 60/2000\n",
      "183/183 - 0s - loss: 0.0067 - val_loss: 0.1695\n",
      "Epoch 61/2000\n",
      "183/183 - 0s - loss: 0.0046 - val_loss: 0.1692\n",
      "Epoch 62/2000\n",
      "183/183 - 0s - loss: 0.0065 - val_loss: 0.1698\n",
      "Epoch 63/2000\n",
      "183/183 - 0s - loss: 0.0061 - val_loss: 0.1705\n",
      "Epoch 64/2000\n",
      "183/183 - 0s - loss: 0.0060 - val_loss: 0.1705\n",
      "Epoch 65/2000\n",
      "183/183 - 0s - loss: 0.0050 - val_loss: 0.1714\n",
      "Epoch 66/2000\n",
      "183/183 - 0s - loss: 0.0051 - val_loss: 0.1732\n",
      "Epoch 67/2000\n",
      "183/183 - 0s - loss: 0.0043 - val_loss: 0.1749\n",
      "Epoch 68/2000\n",
      "183/183 - 0s - loss: 0.0041 - val_loss: 0.1725\n",
      "Epoch 69/2000\n",
      "183/183 - 0s - loss: 0.0063 - val_loss: 0.1706\n",
      "Epoch 70/2000\n",
      "183/183 - 0s - loss: 0.0041 - val_loss: 0.1690\n",
      "Epoch 71/2000\n",
      "183/183 - 0s - loss: 0.0039 - val_loss: 0.1692\n",
      "Epoch 72/2000\n",
      "183/183 - 0s - loss: 0.0041 - val_loss: 0.1700\n",
      "Epoch 73/2000\n",
      "183/183 - 0s - loss: 0.0035 - val_loss: 0.1711\n",
      "Epoch 74/2000\n",
      "183/183 - 0s - loss: 0.0035 - val_loss: 0.1702\n",
      "Epoch 75/2000\n",
      "183/183 - 0s - loss: 0.0063 - val_loss: 0.1713\n",
      "Epoch 76/2000\n",
      "183/183 - 0s - loss: 0.0053 - val_loss: 0.1693\n",
      "Epoch 77/2000\n",
      "183/183 - 0s - loss: 0.0034 - val_loss: 0.1687\n",
      "Epoch 78/2000\n",
      "183/183 - 0s - loss: 0.0042 - val_loss: 0.1674\n",
      "Epoch 79/2000\n",
      "183/183 - 0s - loss: 0.0039 - val_loss: 0.1664\n",
      "Evaluation:\n",
      "\n",
      "MSE: 0.16638057540435347\n",
      "AUC: 0.8399209486166008\n",
      "\n",
      "                                                      CV_1_GS_1  \\\n",
      "AUC                                                    0.852552   \n",
      "Weighted MSE                                            0.15217   \n",
      "Params        {'activation': 'elu', 'batch_size': 64, 'callb...   \n",
      "Features      [Zotu6706, Zotu547, Zotu85, Zotu77, Zotu96, Zo...   \n",
      "\n",
      "                                                      CV_2_GS_1  \\\n",
      "AUC                                                    0.862004   \n",
      "Weighted MSE                                            0.14621   \n",
      "Params        {'activation': 'elu', 'batch_size': 64, 'callb...   \n",
      "Features      [Zotu6706, Zotu547, Zotu85, Zotu77, Zotu96, Zo...   \n",
      "\n",
      "                                                      CV_3_GS_1  \\\n",
      "AUC                                                    0.869565   \n",
      "Weighted MSE                                           0.156664   \n",
      "Params        {'activation': 'elu', 'batch_size': 64, 'callb...   \n",
      "Features      [Zotu6706, Zotu547, Zotu85, Zotu77, Zotu96, Zo...   \n",
      "\n",
      "                                                      CV_4_GS_1  \\\n",
      "AUC                                                    0.810277   \n",
      "Weighted MSE                                           0.186937   \n",
      "Params        {'activation': 'elu', 'batch_size': 64, 'callb...   \n",
      "Features      [Zotu6706, Zotu547, Zotu85, Zotu77, Zotu96, Zo...   \n",
      "\n",
      "                                                      CV_5_GS_1  \n",
      "AUC                                                    0.839921  \n",
      "Weighted MSE                                           0.166381  \n",
      "Params        {'activation': 'elu', 'batch_size': 64, 'callb...  \n",
      "Features      [Zotu6706, Zotu547, Zotu85, Zotu77, Zotu96, Zo...  \n",
      "Directory already exists\n",
      "Reftting on the entire training set with the best found parameters\n",
      "\n",
      "Best found parameters:\n",
      "\n",
      "{'activation': 'elu', 'batch_size': 64, 'callbacks': [<tensorflow.python.keras.callbacks.EarlyStopping object at 0x0000019C07A8A6C8>], 'dropout': 0.3, 'epochs': 2000, 'filters': 10, 'input_shape': None, 'kernel_size': (3, 9), 'learning_rate': 0.001, 'loss': 'mse', 'n_classes': 2, 'n_layers': 1, 'pool_size': (2, 2), 'verbose': 2}\n",
      "Samples X_train: 205\n",
      "Samples X_test: 58\n",
      "Labels in y_train: [102. 103.]\n",
      "\n",
      "Rebuilt model and thus reinitialized\n",
      "Train on 205 samples, validate on 23 samples\n",
      "Epoch 1/2000\n",
      "205/205 - 1s - loss: 0.2520 - val_loss: 0.2392\n",
      "Epoch 2/2000\n",
      "205/205 - 0s - loss: 0.2400 - val_loss: 0.2352\n",
      "Epoch 3/2000\n",
      "205/205 - 0s - loss: 0.2259 - val_loss: 0.2322\n",
      "Epoch 4/2000\n",
      "205/205 - 0s - loss: 0.2109 - val_loss: 0.2187\n",
      "Epoch 5/2000\n",
      "205/205 - 0s - loss: 0.1977 - val_loss: 0.2168\n",
      "Epoch 6/2000\n",
      "205/205 - 0s - loss: 0.1854 - val_loss: 0.2058\n",
      "Epoch 7/2000\n",
      "205/205 - 0s - loss: 0.1701 - val_loss: 0.1971\n",
      "Epoch 8/2000\n",
      "205/205 - 0s - loss: 0.1686 - val_loss: 0.1934\n",
      "Epoch 9/2000\n",
      "205/205 - 0s - loss: 0.1449 - val_loss: 0.1921\n",
      "Epoch 10/2000\n",
      "205/205 - 0s - loss: 0.1477 - val_loss: 0.1872\n",
      "Epoch 11/2000\n",
      "205/205 - 0s - loss: 0.1307 - val_loss: 0.1855\n",
      "Epoch 12/2000\n",
      "205/205 - 0s - loss: 0.1165 - val_loss: 0.1744\n",
      "Epoch 13/2000\n",
      "205/205 - 0s - loss: 0.1166 - val_loss: 0.1730\n",
      "Epoch 14/2000\n",
      "205/205 - 0s - loss: 0.1100 - val_loss: 0.1802\n",
      "Epoch 15/2000\n",
      "205/205 - 0s - loss: 0.0983 - val_loss: 0.1751\n",
      "Epoch 16/2000\n",
      "205/205 - 0s - loss: 0.0854 - val_loss: 0.1729\n",
      "Epoch 17/2000\n",
      "205/205 - 0s - loss: 0.0822 - val_loss: 0.1704\n",
      "Epoch 18/2000\n",
      "205/205 - 0s - loss: 0.0794 - val_loss: 0.1691\n",
      "Epoch 19/2000\n",
      "205/205 - 0s - loss: 0.0690 - val_loss: 0.1736\n",
      "Epoch 20/2000\n",
      "205/205 - 0s - loss: 0.0660 - val_loss: 0.1683\n",
      "Epoch 21/2000\n",
      "205/205 - 0s - loss: 0.0599 - val_loss: 0.1641\n",
      "Epoch 22/2000\n",
      "205/205 - 0s - loss: 0.0575 - val_loss: 0.1647\n",
      "Epoch 23/2000\n",
      "205/205 - 0s - loss: 0.0535 - val_loss: 0.1762\n",
      "Epoch 24/2000\n",
      "205/205 - 0s - loss: 0.0485 - val_loss: 0.1707\n",
      "Epoch 25/2000\n",
      "205/205 - 0s - loss: 0.0428 - val_loss: 0.1640\n",
      "Epoch 26/2000\n",
      "205/205 - 0s - loss: 0.0421 - val_loss: 0.1635\n",
      "Epoch 27/2000\n",
      "205/205 - 0s - loss: 0.0381 - val_loss: 0.1777\n",
      "Epoch 28/2000\n",
      "205/205 - 0s - loss: 0.0396 - val_loss: 0.1701\n",
      "Epoch 29/2000\n",
      "205/205 - 0s - loss: 0.0344 - val_loss: 0.1643\n",
      "Epoch 30/2000\n",
      "205/205 - 0s - loss: 0.0282 - val_loss: 0.1643\n",
      "Epoch 31/2000\n",
      "205/205 - 0s - loss: 0.0276 - val_loss: 0.1717\n",
      "Epoch 32/2000\n",
      "205/205 - 0s - loss: 0.0271 - val_loss: 0.1723\n",
      "Epoch 33/2000\n",
      "205/205 - 0s - loss: 0.0237 - val_loss: 0.1648\n",
      "Epoch 34/2000\n",
      "205/205 - 0s - loss: 0.0259 - val_loss: 0.1638\n",
      "Epoch 35/2000\n",
      "205/205 - 0s - loss: 0.0210 - val_loss: 0.1649\n",
      "Epoch 36/2000\n",
      "205/205 - 0s - loss: 0.0192 - val_loss: 0.1680\n",
      "Epoch 37/2000\n",
      "205/205 - 0s - loss: 0.0242 - val_loss: 0.1661\n",
      "Epoch 38/2000\n",
      "205/205 - 0s - loss: 0.0192 - val_loss: 0.1658\n",
      "Epoch 39/2000\n",
      "205/205 - 0s - loss: 0.0179 - val_loss: 0.1666\n",
      "Epoch 40/2000\n",
      "205/205 - 0s - loss: 0.0150 - val_loss: 0.1688\n",
      "Epoch 41/2000\n",
      "205/205 - 0s - loss: 0.0134 - val_loss: 0.1732\n",
      "Epoch 42/2000\n",
      "205/205 - 0s - loss: 0.0136 - val_loss: 0.1738\n",
      "Epoch 43/2000\n",
      "205/205 - 0s - loss: 0.0137 - val_loss: 0.1706\n",
      "Epoch 44/2000\n",
      "205/205 - 0s - loss: 0.0150 - val_loss: 0.1716\n",
      "Epoch 45/2000\n",
      "205/205 - 0s - loss: 0.0123 - val_loss: 0.1723\n",
      "Epoch 46/2000\n",
      "205/205 - 0s - loss: 0.0127 - val_loss: 0.1726\n",
      "Epoch 47/2000\n",
      "205/205 - 0s - loss: 0.0126 - val_loss: 0.1735\n",
      "Epoch 48/2000\n",
      "205/205 - 0s - loss: 0.0149 - val_loss: 0.1748\n",
      "Epoch 49/2000\n",
      "205/205 - 0s - loss: 0.0086 - val_loss: 0.1738\n",
      "Epoch 50/2000\n",
      "205/205 - 0s - loss: 0.0104 - val_loss: 0.1747\n",
      "Epoch 51/2000\n",
      "205/205 - 0s - loss: 0.0078 - val_loss: 0.1769\n",
      "Epoch 52/2000\n",
      "205/205 - 0s - loss: 0.0074 - val_loss: 0.1806\n",
      "Epoch 53/2000\n",
      "205/205 - 0s - loss: 0.0080 - val_loss: 0.1784\n",
      "Epoch 54/2000\n",
      "205/205 - 0s - loss: 0.0085 - val_loss: 0.1757\n",
      "Epoch 55/2000\n",
      "205/205 - 0s - loss: 0.0087 - val_loss: 0.1748\n",
      "Epoch 56/2000\n",
      "205/205 - 0s - loss: 0.0096 - val_loss: 0.1766\n",
      "Epoch 57/2000\n",
      "205/205 - 0s - loss: 0.0072 - val_loss: 0.1794\n",
      "Epoch 58/2000\n",
      "205/205 - 0s - loss: 0.0068 - val_loss: 0.1787\n",
      "Epoch 59/2000\n",
      "205/205 - 0s - loss: 0.0069 - val_loss: 0.1784\n",
      "Epoch 60/2000\n",
      "205/205 - 0s - loss: 0.0074 - val_loss: 0.1791\n",
      "Epoch 61/2000\n",
      "205/205 - 0s - loss: 0.0077 - val_loss: 0.1801\n",
      "Epoch 62/2000\n",
      "205/205 - 0s - loss: 0.0065 - val_loss: 0.1784\n",
      "Epoch 63/2000\n",
      "205/205 - 0s - loss: 0.0070 - val_loss: 0.1783\n",
      "Epoch 64/2000\n",
      "205/205 - 0s - loss: 0.0061 - val_loss: 0.1816\n",
      "Epoch 65/2000\n",
      "205/205 - 0s - loss: 0.0065 - val_loss: 0.1827\n",
      "Epoch 66/2000\n",
      "205/205 - 0s - loss: 0.0077 - val_loss: 0.1826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Flori\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:218: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n"
     ]
    }
   ],
   "source": [
    "skf=StratifiedKFold(n_splits = n_cv_folds)\n",
    "\n",
    "param_grid = {\n",
    "    'input_shape': [None],\n",
    "    'n_layers': [1],\n",
    "    'filters': [10],\n",
    "    'kernel_size': [(3, 9)],\n",
    "    'pool_size': [(2, 2)],\n",
    "    'activation': ['elu'],\n",
    "    'n_classes': [2],\n",
    "    'learning_rate': [0.001],\n",
    "    'loss': ['mse'],\n",
    "    'dropout': [0.3],\n",
    "    'batch_size': [64],\n",
    "    'epochs': [2000],\n",
    "    'callbacks': [[EarlyStopping(patience=40)]],\n",
    "    'verbose': [2]\n",
    "}\n",
    "\n",
    "grid_size = 1\n",
    "for key, value in param_grid.items():\n",
    "    grid_size *= len(value)\n",
    "fit_keys = ['batch_size', 'epochs', 'callbacks', 'verbose']\n",
    "\n",
    "###############################################################\n",
    "# Starting stability selection loop\n",
    "###############################################################\n",
    "test_stat_df = pd.DataFrame(index=[\"AUC\", \"Weighted MSE\", \"Params\", \"Features\"], columns=[i+1 for i in range(n_shuffles)])\n",
    "shuffle_counter = 0\n",
    "\n",
    "print(X.shape)\n",
    "print(type(X))\n",
    "\n",
    "n_values = np.max(y) + 1\n",
    "labels_oh = np.eye(n_values)[y]\n",
    "\n",
    "for samples,test in StratShufSpl.split(X, y):\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"Beginning stability selection iteration {}\".format(shuffle_counter))\n",
    "    print(\"----------------------------------------------------------------\\n\")\n",
    "    shuffle_counter+=1\n",
    "    X_train, X_test = X.iloc[samples], X.iloc[test]\n",
    "    y_train, y_test=y[samples], y[test]\n",
    "\n",
    "    # Creating a dataframe for storing the results\n",
    "    cv_list = [\"CV_{}_GS_{}\".format(str(i+1), str(j+1)) for i in range(n_cv_folds) for j in range(len(ParameterGrid(param_grid)))]\n",
    "    stat_df = pd.DataFrame(index=[\"AUC\", \"Weighted MSE\", \"Params\", \"Features\"], columns=cv_list)\n",
    "\n",
    "    # Performing the grid search CV\n",
    "    n_candidates = n_cv_folds * grid_size\n",
    "    cv_fold = 0\n",
    "    candidate_counter = 0\n",
    "    print('Performing GridSearchCV for {} candidates\\n\\n'.format(n_candidates))\n",
    "    for train_index, test_index in skf.split(X_train, y_train):\n",
    "        print(\"----------------------------------------------------------------\")\n",
    "        print(\"Beginning cross validation for candidate number {}\".format(candidate_counter))\n",
    "        print(\"----------------------------------------------------------------\\n\")\n",
    "        #################################################################\n",
    "        # Select and format training and testing sets\n",
    "        #################################################################\n",
    "        cv_fold += 1\n",
    "        gs_it = 0\n",
    "\n",
    "        train_X, val_X = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        train_index = samples[train_index]\n",
    "        test_index = samples[test_index]\n",
    "        train_y, val_y = labels_oh[train_index,:], labels_oh[test_index,:]\n",
    "        class_frequencies_train = np.sum(train_y, axis = 0)/len(train_y)\n",
    "        class_frequencies_val = np.sum(val_y, axis = 0)/len(val_y)\n",
    "        print('Total size of the training set: {}'.format(len(train_X)))\n",
    "        print('Total size of the validation set: {}'.format(len(val_X)))\n",
    "        print('Class frequencies in the training set: {}'.format(class_frequencies_train))\n",
    "        print('Class frequencies in the validation set: {}\\n'.format(class_frequencies_val))\n",
    "     \n",
    "        \n",
    "        # Build tree\n",
    "        print(\"----------------------------------------------------------------\")\n",
    "        print(\"Beginning the tree building procedure\")\n",
    "        print(\"----------------------------------------------------------------\\n\")\n",
    "\n",
    "        tree_builder = TreeBuilder(tree_path)\n",
    "        tree_builder = tree_builder.fit(train_X, train_y)\n",
    "        train_X = tree_builder.transform(train_X)\n",
    "        val_X = tree_builder.transform(val_X) \n",
    "\n",
    "        for g in ParameterGrid(param_grid):\n",
    "            candidate_counter += 1\n",
    "            print('Fitting candidate number {} in shuffle {} with parameters\\n'.format(candidate_counter, shuffle_counter))\n",
    "            print(g)\n",
    "            gs_it += 1\n",
    "            params = g.copy()\n",
    "            fit_params = {key: g.pop(key) for key in fit_keys}\n",
    "\n",
    "            num_train_samples = train_X.shape[0]\n",
    "            num_test_samples = val_X.shape[0]\n",
    "            tree_row = train_X.shape[1]\n",
    "            tree_col = train_X.shape[2]\n",
    "\n",
    "            g['input_shape'] = (tree_row, tree_col)\n",
    "        \n",
    "            fit_params['x'] = train_X\n",
    "            fit_params['y'] = train_y\n",
    "            fit_params['validation_data'] = (val_X, val_y)\n",
    "            \n",
    "            # Seting model parameters and fitting\n",
    "            model = build_model(**g)\n",
    "            model.fit(**fit_params)\n",
    "\n",
    "            # Evaluation\n",
    "            print('Evaluation:\\n')\n",
    "            val_preds = model.predict(val_X)\n",
    "            auc_score = roc_auc_score(val_y, val_preds)\n",
    "            mse = mean_squared_error(val_y, val_preds)\n",
    "            #mse, auc = model.evaluate(x = val_X, y = val_y, verbose = 0)\n",
    "            print('MSE: {}'.format(mse))\n",
    "            print('AUC: {}\\n'.format(auc_score))\n",
    "            \n",
    "            # Storing stats in dataframe\n",
    "            stat_df.loc[\"Weighted MSE\"][\"CV_{}_GS_{}\".format(cv_fold, gs_it)] = mse\n",
    "            stat_df.loc[\"AUC\"][\"CV_{}_GS_{}\".format(cv_fold, gs_it)] = auc_score\n",
    "            stat_df.loc[\"Params\"][\"CV_{}_GS_{}\".format(cv_fold, gs_it)] = params\n",
    "            stat_df.loc[\"Features\"][\"CV_{}_GS_{}\".format(cv_fold, gs_it)] = tree_builder.features\n",
    "\n",
    "            # Resetting model weights and clearing the session\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "    print(stat_df)\n",
    "    # End of GS_CV\n",
    "    # Find best params according to lowest weighted MSE\n",
    "    # Refit model on train + val with best params\n",
    "    # Report all scores on test\n",
    "    # Save in test_stat_df\n",
    "\n",
    "    # Saving the results\n",
    "    try:\n",
    "        os.mkdir('output_data')\n",
    "    except OSError as error:\n",
    "        print('Directory already exists')\n",
    "\n",
    "    stat_df.to_csv('output_data/validation_results_{}.csv'.format(shuffle_counter))\n",
    "\n",
    "\n",
    "    # Reftting on the entire training set with the best found parameters\n",
    "    print('Reftting on the entire training set with the best found parameters\\n')\n",
    "    tf.keras.backend.clear_session()\n",
    "    grouped_df, params = group_by_params(stat_df, num_combinations = grid_size)\n",
    "    grouped_df.to_csv('output_data/grouped_validation_results_{}.csv'.format(shuffle_counter))\n",
    "    best_score_index = np.argmin(list(grouped_df.loc['Weighted MSE']))\n",
    "    best_params = params[best_score_index]\n",
    "    print('Best found parameters:\\n')\n",
    "    print(best_params)\n",
    "    X_train = np.log(X_train + 1)\n",
    "    X_test = np.log(X_test + 1)\n",
    "    y_train, y_test = labels_oh[samples,:], labels_oh[test,:]\n",
    "\n",
    "    \n",
    "    # Build tree\n",
    "    tree_builder = TreeBuilder(tree_path)\n",
    "    tree_builder = tree_builder.fit(X_train, y_train)\n",
    "    X_train = tree_builder.transform(X_train)\n",
    "    X_test = tree_builder.transform(X_test) \n",
    "\n",
    "    num_train_samples = X_train.shape[0]\n",
    "    num_test_samples = X_test.shape[0]\n",
    "    tree_row = X_train.shape[1]\n",
    "    tree_col = X_train.shape[2]\n",
    "\n",
    "    fit_params = {key: best_params.pop(key) for key in fit_keys}\n",
    "\n",
    "    # Splitting the data in train and val for early stopping\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "                                                    stratify=y_train, \n",
    "                                                   test_size=0.1)\n",
    "    fit_params['x'] = X_train\n",
    "    fit_params['y'] = y_train\n",
    "    fit_params['validation_data'] = (X_val, y_val)\n",
    "    best_params['input_shape'] = (tree_row, tree_col)\n",
    "\n",
    "    print('Samples X_train: {}'.format(len(X_train)))\n",
    "    print('Samples X_test: {}'.format(len(X_test)))\n",
    "    print('Labels in y_train: {}\\n'.format(np.sum(y_train, axis = 0)))\n",
    "\n",
    "    #test_untrained_weights = model.get_weights().copy()\n",
    "    model = build_model(**best_params)\n",
    "    print('Rebuilt model and thus reinitialized')\n",
    "    model.fit(**fit_params)\n",
    "    \n",
    "    preds = model.predict(X_test)[:, 0]\n",
    "    y_test = y_test[:, 0]\n",
    "    np.save('output_data/test_preds.npy', preds)\n",
    "    np.save('output_data/test_y.npy', y_test)\n",
    "\n",
    "    ### ONLY FOR A TEST\n",
    "    #np.save(\"X_train.npy\", X_train)\n",
    "    #model.save(\"test_model\")\n",
    "\n",
    "    final_params = {**best_params, **fit_params}\n",
    "    del final_params['x']\n",
    "    del final_params['y']\n",
    "    del final_params['validation_data']\n",
    "\n",
    "    mse = model.evaluate(x = X_test, y = y_test, verbose = 0)\n",
    "    auc_score = roc_auc_score(y_test, preds)\n",
    "\n",
    "    test_stat_df.loc[\"Weighted MSE\"][shuffle_counter] = mse\n",
    "    test_stat_df.loc[\"AUC\"][shuffle_counter] = auc_score\n",
    "    test_stat_df.loc[\"Features\"][shuffle_counter] = tree_builder.features\n",
    "    test_stat_df.loc[\"Params\"][shuffle_counter] = final_params\n",
    "\n",
    "    # Storing the FPR, TPR and thresholds for creating an AUC plot\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, preds, pos_label=1)\n",
    "    auc_dict[\"FPR\"].append(fpr)\n",
    "    auc_dict[\"TPR\"].append(tpr)\n",
    "    auc_dict[\"Thresholds\"].append(thresholds)\n",
    "    auc_dict[\"AUC\"].append(auc_score)\n",
    "    \n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    aucs.append(auc_roc1)\n",
    "\n",
    "\n",
    "    # Resetting model weights and clearing the session\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=1, color='r',\n",
    "         label='Random guessing', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=1, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color = 'grey', alpha = .2,\n",
    "                 label = r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.savefig('output_data/auc_avg.pdf', bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
